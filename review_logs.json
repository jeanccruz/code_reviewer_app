[
  {
    "name": "teste_1",
    "timestamp": "2025-06-15T11:38:08-07:00",
    "input_type": "file",
    "content": "diff --git a/my_spark_job.py b/my_spark_job.py\nindex 9f12b6e..cf38d12 100644\n--- a/my_spark_job.py\n+++ b/my_spark_job.py\n@@ -1,9 +1,13 @@\n-from pyspark.sql import SparkSession\n-from pyspark.sql.functions import col\n+from pyspark.sql import SparkSession\n+from pyspark.sql.functions import col,lit,when,current_date\n \n def main():\n     spark = SparkSession.builder \\\n         .appName(\"MySparkJob\") \\\n         .getOrCreate()\n \n-    df = spark.read.csv(\"s3://my-bucket/input.csv\", header=True, inferSchema=True)\n-    df_filtered = df.filter(col(\"status\") == \"active\")\n+    df = spark.read.option(\"multiline\", True).json(\"s3://my-bucket/input.json\")\n+    df_filtered = df.filter(when(col(\"status\") == \"active\", True).otherwise(False))\n+    df_filtered = df_filtered.withColumn(\"ingestion_date\", lit(current_date()))\n+    records = df_filtered.collect() # BAD: .collect() on full dataset (OOM risk)\n+    print(\"Number of records: \", len(records))  \n+    df_filtered.write.mode(\"overwrite\").parquet(\"s3://my-bucket/output/\") # NO partitioning\n+\n+    print(undeclared_variable) # SYNTAX ERROR: name 'undeclared_variable' is not defined\n+\n+    spark.stop()",
    "model_output": {
      "executive_summary": "The code changes introduce several modifications to a Spark job, including reading from a JSON file instead of CSV, using a `when` clause for filtering, adding an `ingestion_date` column, collecting the entire filtered DataFrame to the driver node, writing the DataFrame to a Parquet file, and introducing a syntax error by referencing an undefined variable. The most significant concern is the use of `.collect()` on a potentially large DataFrame, which can lead to out-of-memory errors on the driver node. The lack of partitioning when writing to Parquet is also suboptimal for performance.  The use of `when` for filtering is also inefficient. Also a syntax error is included for demonstration.",
      "syntax_review": [
        {
          "line": 17,
          "issue": "NameError: name 'undeclared_variable' is not defined",
          "severity": "CRITICAL",
          "recommendation": "This line will cause the program to crash. Remove the line `print(undeclared_variable)` or define the variable before using it."
        }
      ],
      "code_style": [
        {
          "line": 3,
          "issue": "Import statement unnecessarily long.",
          "severity": "MINOR",
          "recommendation": "Consider importing individual functions, e.g., `from pyspark.sql.functions import col, lit, when, current_date` for better readability, but this change is already done."
        },
        {
          "issue": "No docstrings are provided.",
          "severity": "MINOR",
          "recommendation": "Add docstrings to functions to explain their purpose, arguments, and return values. For example, add a docstring to `main()` describing what the job does."
        },
        {
          "issue": "Inconsistent spacing around operators.",
          "severity": "MINOR",
          "recommendation": "Ensure consistent spacing around operators such as `=` and `==`. While the code is generally readable, using a linter like `flake8` with PEP 8 checks will catch minor style issues automatically."
        }
      ],
      "performance_analysis": [
        {
          "line": 14,
          "issue": "Using `.collect()` on the entire DataFrame.",
          "severity": "CRITICAL",
          "recommendation": "`.collect()` brings the entire DataFrame to the driver node.  This can easily lead to out-of-memory errors if the DataFrame is large.  Avoid this.  If you need to inspect a few rows, use `.take(n)` or `.limit(n).collect()` for a small number of rows.  If you need to aggregate data, use Spark's aggregation functions (e.g., `count()`, `sum()`, `groupBy()`). If needing to print the number of active records, count the records using `df_filtered.count()` which performs a distributed count and thus does not load the entire dataframe into the driver node's memory.",
          "alternatives": [
            "`df_filtered.take(10)`",
            "`df_filtered.limit(10).collect()`",
            "`df_filtered.count()`"
          ]
        },
        {
          "line": 15,
          "issue": "Writing to Parquet without partitioning.",
          "severity": "MAJOR",
          "recommendation": "Writing to Parquet without partitioning can lead to performance issues when querying the data later.  Partition the data based on a column that is frequently used in queries (e.g., a date column, a category column).  Use `df_filtered.write.partitionBy(\"partition_column\").mode(\"overwrite\").parquet(\"s3://my-bucket/output/\")`.",
          "alternatives": [
            "`df_filtered.write.partitionBy(\"year\", \"month\").mode(\"overwrite\").parquet(\"s3://my-bucket/output/\")` (Example with multiple columns)"
          ]
        },
        {
          "line": 12,
          "issue": "Using `when` to filter is less efficient than a direct filter.",
          "severity": "MINOR",
          "recommendation": "The `when` clause creates a new column with boolean values, which is then filtered. A direct filter is more efficient.  Use `df.filter(col(\"status\") == \"active\")` instead.",
          "alternatives": [
            "`df.filter(col(\"status\") == \"active\")`"
          ]
        },
        {
          "issue": "Reading JSON with `multiline=True` can be inefficient for large JSON files.",
          "severity": "MEDIUM",
          "recommendation": "Using `multiline=True` reads the entire JSON file into a single record if the file has a top-level JSON object. If the JSON file contains one JSON object per line (JSON Lines format), remove `multiline=True` for better performance and scalability.",
          "alternatives": [
            "Remove `option(\"multiline\", True)` if the JSON file is in JSON Lines format."
          ]
        }
      ]
    },
    "error": null
  },
  {
    "name": "teste-2",
    "timestamp": "2025-06-15T11:38:08-07:00",
    "input_type": "url",
    "content": "diff --git a/jobs/user_data_etl.py b/jobs/user_data_etl.py\nindex b1e35c2..d7f8fa9 100644\n--- a/jobs/user_data_etl.py\n+++ b/jobs/user_data_etl.py\n@@ -1,11 +1,28 @@\n-from pyspark.sql import SparkSession\n-from pyspark.sql.functions import col\n+from pyspark.sql import SparkSession\n+from pyspark.sql.functions import col, udf, lower, upper\n+from pyspark.sql.types import StringType\n+\n+import time\n+\n+spark = SparkSession.builder.appName(\"UserETL\").getOrCreate()  # BAD: global SparkSession\n\n-def run():\n-    spark = SparkSession.builder.appName(\"UserETL\").getOrCreate()\n-    df = spark.read.csv(\"s3://my-bucket/users.csv\", header=True, inferSchema=True)\n-    df_active = df.filter(col(\"status\") == \"active\")\n-    df_active.write.parquet(\"s3://my-bucket/output/users_active\", mode=\"overwrite\")\n+def main():\n+    data = spark.read.csv(\"/home/jean/Desktop/test.csv\", header=True)  # \u274c Local path, hardcoded\n+\n+    data.createOrReplaceTempView(\"users_temp\")\n+    data_again = spark.sql(\"SELECT * FROM users_temp\")  # \u274c Duplicate copy\n+\n+    print(\"Total rows:\", data_again.count())  # \u274c Avoid count() in driver for big data\n+\n+    data_cleaned = data_again.withColumn(\"username\", upper(col(\"username\")))\n+    data_cleaned = data_cleaned.withColumn(\"email\", lower(col(\"email\")))\n+\n+    def my_clean(x):\n+     return x.strip()  # \u274c No null check, incorrect indentation\n+\n+    clean_udf = udf(my_clean, StringType())\n+    data_cleaned = data_cleaned.withColumn(\"username\", clean_udf(col(\"username\")))\n+\n+    for row in data_cleaned.collect():  # \u274c collect() + for loop = performance disaster\n+     print(row)\n \n-if __name__ == \"__main__\":\n-    run()\n+    data_cleaned.write.mode(\"overwrite\").parquet(\"/tmp/users_cleaned\")  # \u274c No partitioning\n+\n+if __name__==\"__main__\":main()  # \u274c PEP8 formatting and one-liner\n",
    "model_output": {
      "executive_summary": "The provided code snippet implements a basic user data ETL process using Apache Spark. However, it contains several critical issues related to Spark performance, code style, and best practices. The code reads a CSV file, performs some simple data cleaning and transformation operations, and writes the result to a Parquet file. The major concerns revolve around using `collect()` and iterating in the driver, defining a global SparkSession, using local files instead of distributed storage, and not partitioning the output data. These issues can lead to significant performance bottlenecks and scalability problems, especially when dealing with large datasets.",
      "syntax_review": {
        "issues": [
          "Incorrect indentation within the `my_clean` function.",
          "Missing type hints for function definitions, impacting readability and maintainability.",
          "One-liner `if __name__==\"__main__\":main()` violates PEP8 and reduces readability."
        ],
        "suggestions": [
          "Correct indentation within the `my_clean` function to ensure proper execution.",
          "Add type hints to improve code readability and maintainability.",
          "Separate the `if __name__ == '__main__'` block for better readability."
        ]
      },
      "code_style": {
        "issues": [
          "PEP8 violations: Inconsistent spacing around operators, one-liner `if __name__ == '__main__'` statement.",
          "Lack of docstrings for functions and modules.  Documenting functions with clear explanations is crucial for maintainability.",
          "Inconsistent naming conventions (e.g., some variables use snake_case, others don't).",
          "Global SparkSession variable is generally discouraged.",
          "The comment style is inconsistent."
        ],
        "suggestions": [
          "Use a code formatter like `black` or `autopep8` to ensure PEP8 compliance.",
          "Add docstrings to all functions and the module to explain their purpose and usage.",
          "Adopt a consistent naming convention (e.g., snake_case for all variables and functions).",
          "Create SparkSession within the function where it's needed, not globally.",
          "Ensure all comments are clear and concise."
        ]
      },
      "performance_analysis": {
        "issues": [
          "Using a global SparkSession can lead to unexpected behavior and resource contention in larger applications.",
          "Reading data from a local file path (`/home/jean/Desktop/test.csv`) defeats the purpose of using Spark, as it limits parallelism and scalability.  Spark is designed to work with distributed storage like HDFS, S3, or Azure Blob Storage.",
          "Creating a temporary view (`data.createOrReplaceTempView(\"users_temp\")`) and then immediately querying it (`spark.sql(\"SELECT * FROM users_temp\")`) creates a redundant copy of the DataFrame without any transformation, which is inefficient.",
          "`data_again.count()` called on the driver can be extremely slow and potentially crash the driver node for large datasets. The entire dataset must be transferred to the driver.",
          "The UDF `my_clean` does not handle null values which can lead to errors. It is also recommended to use built-in spark functions instead of UDFs where possible to avoid serialization/deserialization overhead.",
          "The `collect()` operation followed by a `for` loop brings the entire DataFrame to the driver's memory. This is extremely inefficient and can lead to out-of-memory errors for large datasets. Spark is designed for distributed processing; avoid pulling data to the driver whenever possible.",
          "Writing data to a single Parquet file (`/tmp/users_cleaned`) without partitioning negates the benefits of Spark's parallelism when reading the data later. Partitioning allows Spark to distribute the read workload across multiple executors.",
          "Multiple `withColumn` calls can be chained but can be optimized by performing all transformations in one `select` statement."
        ],
        "suggestions": [
          "Create SparkSession within the `main` function rather than globally.",
          "Read data from a distributed storage system like S3, HDFS, or Azure Blob Storage to enable parallel processing.",
          "Remove the redundant temporary view and query. Operate directly on the initial `data` DataFrame.",
          "Avoid using `count()` on the driver for large datasets. If a count is truly needed, consider using an approximate count or sampling techniques.",
          "Modify the `my_clean` UDF to handle null values gracefully (e.g., by returning an empty string or using `None`). Consider using `regexp_replace` instead of a UDF, which could be more efficient.",
          "Completely eliminate the `collect()` operation and the `for` loop.  If you need to inspect data, use `show()` to display a sample of the DataFrame.",
          "Partition the output data when writing to Parquet using the `partitionBy()` method, based on a relevant column (e.g., `status`, `country`). This significantly improves read performance in subsequent queries.",
          "Chain multiple `withColumn` transformations into one expression or use Spark SQL to apply multiple transformation steps together to reduce stages of execution."
        ]
      }
    },
    "error": null
  },
  {
    "name": "teste_3",
    "timestamp": "2025-06-15T11:38:08-07:00",
    "input_type": "file",
    "content": "diff --git a/jobs/user_data_etl.py b/jobs/user_data_etl.py\nindex b1e35c2..d7f8fa9 100644\n--- a/jobs/user_data_etl.py\n+++ b/jobs/user_data_etl.py\n@@ -1,11 +1,28 @@\n-from pyspark.sql import SparkSession\n-from pyspark.sql.functions import col\n+from pyspark.sql import SparkSession\n+from pyspark.sql.functions import col, udf, lower, upper\n+from pyspark.sql.types import StringType\n+\n+import time\n+\n+spark = SparkSession.builder.appName(\"UserETL\").getOrCreate()  # BAD: global SparkSession\n\n-def run():\n-    spark = SparkSession.builder.appName(\"UserETL\").getOrCreate()\n-    df = spark.read.csv(\"s3://my-bucket/users.csv\", header=True, inferSchema=True)\n-    df_active = df.filter(col(\"status\") == \"active\")\n-    df_active.write.parquet(\"s3://my-bucket/output/users_active\", mode=\"overwrite\")\n+def main():\n+    data = spark.read.csv(\"/home/jean/Desktop/test.csv\", header=True)  # \u274c Local path, hardcoded\n+\n+    data.createOrReplaceTempView(\"users_temp\")\n+    data_again = spark.sql(\"SELECT * FROM users_temp\")  # \u274c Duplicate copy\n+\n+    print(\"Total rows:\", data_again.count())  # \u274c Avoid count() in driver for big data\n+\n+    data_cleaned = data_again.withColumn(\"username\", upper(col(\"username\")))\n+    data_cleaned = data_cleaned.withColumn(\"email\", lower(col(\"email\")))\n+\n+    def my_clean(x):\n+     return x.strip()  # \u274c No null check, incorrect indentation\n+\n+    clean_udf = udf(my_clean, StringType())\n+    data_cleaned = data_cleaned.withColumn(\"username\", clean_udf(col(\"username\")))\n+\n+    for row in data_cleaned.collect():  # \u274c collect() + for loop = performance disaster\n+     print(row)\n \n-if __name__ == \"__main__\":\n-    run()\n+    data_cleaned.write.mode(\"overwrite\").parquet(\"/tmp/users_cleaned\")  # \u274c No partitioning\n+\n+if __name__==\"__main__\":main()  # \u274c PEP8 formatting and one-liner",
    "model_output": {
      "executive_summary": "The provided code snippet demonstrates a basic ETL process using PySpark. However, it contains several critical flaws regarding performance, code style, and best practices for Spark development. It reads data, performs transformations (uppercase username, lowercase email, strip whitespace), and writes the result.  The major issues are related to reading from a local file, using `collect()` followed by a loop, using `count()` on the driver, inefficient string cleaning with a UDF,  lack of data partitioning and incorrect indentation. Furthermore, the SparkSession is defined globally which is an anti-pattern.",
      "syntax_review": {
        "comments": "The code contains basic syntax errors, such as incorrect indentation inside the `my_clean` function.  There is also an issue with the final line, `if __name__==\"__main__\":main()`, which is not PEP8 compliant.",
        "suggestions": "Correct the indentation within the `my_clean` function. Conform the conditional statement to PEP8 standards with spaces around operators: `if __name__ == \"__main__\": main()`"
      },
      "code_style": {
        "comments": "The code violates PEP8 in several places. Specifically, there is a lack of spacing around operators and assignment in `if __name__==\"__main__\":main()`.  There is also a global SparkSession and the indentation in `my_clean` function is incorrect.",
        "suggestions": "1.  Define the SparkSession within the `main` function instead of globally.  2.  Ensure correct indentation within the `my_clean` function. 3. Add spaces around operators in the conditional statement. 4. Consider using `pylint` or `flake8` to automatically check PEP8 compliance."
      },
      "performance_analysis": {
        "comments": "The code exhibits several serious performance bottlenecks. The use of `data.createOrReplaceTempView` followed by `spark.sql` to select all data does not introduce any advantage. Furthermore, using `count()` on the driver node can be very slow for large datasets, because it pulls ALL the data to the driver. Also, applying a UDF for simple string cleaning is inefficient. `collect()` and iterating through the result will be a huge bottleneck as it moves the entire dataset to the driver node. There is no partitioning when writing out the data. The local file path `\"/home/jean/Desktop/test.csv\"` will not scale, and is unusable in a distributed environment.",
        "suggestions": "1.  Avoid unnecessary views and SQL queries unless complex logic is involved. 2.  Avoid `count()` on the driver node, and use aggregate operations within Spark. 3.  Use built-in Spark functions like `trim()` instead of UDFs for string manipulation.  4.  Avoid `collect()` and the subsequent loop at all costs for large datasets. Perform operations using Spark transformations instead. 5. Partition the output data based on relevant columns using `partitionBy()` when writing to Parquet. 6. Read data from a distributed storage system like S3 or HDFS instead of a local file.  7.  Consider broadcasting smaller lookup tables if using joins."
      }
    },
    "error": null
  },
  {
    "name": "teste_4",
    "timestamp": "2025-06-15T11:38:08-07:00",
    "input_type": "file",
    "content": "diff --git a/jobs/user_data_etl.py b/jobs/user_data_etl.py\nindex b1e35c2..d7f8fa9 100644\n--- a/jobs/user_data_etl.py\n+++ b/jobs/user_data_etl.py\n@@ -1,11 +1,28 @@\n-from pyspark.sql import SparkSession\n-from pyspark.sql.functions import col\n+from pyspark.sql import SparkSession\n+from pyspark.sql.functions import col, udf, lower, upper\n+from pyspark.sql.types import StringType\n+\n+import time\n+\n+spark = SparkSession.builder.appName(\"UserETL\").getOrCreate()\n\n-def run():\n-    spark = SparkSession.builder.appName(\"UserETL\").getOrCreate()\n-    df = spark.read.csv(\"s3://my-bucket/users.csv\", header=True, inferSchema=True)\n-    df_active = df.filter(col(\"status\") == \"active\")\n-    df_active.write.parquet(\"s3://my-bucket/output/users_active\", mode=\"overwrite\")\n+def main():\n+    data = spark.read.csv(\"/home/jean/Desktop/test.csv\", header=True)\n+\n+    data.createOrReplaceTempView(\"users_temp\")\n+    data_again = spark.sql(\"SELECT * FROM users_temp\")\n+\n+    print(\"Total rows:\", data_again.count())\n+\n+    data_cleaned = data_again.withColumn(\"username\", upper(col(\"username\")))\n+    data_cleaned = data_cleaned.withColumn(\"email\", lower(col(\"email\")))\n+\n+    def my_clean(x):\n+     return x.strip()\n+\n+    clean_udf = udf(my_clean, StringType())\n+    data_cleaned = data_cleaned.withColumn(\"username\", clean_udf(col(\"username\")))\n+\n+    for row in data_cleaned.collect():\n+     print(row)\n \n-if __name__ == \"__main__\":\n-    run()\n+    data_cleaned.write.mode(\"overwrite\").parquet(\"/tmp/users_cleaned\")\n+\n+if __name__==\"__main__\":main()",
    "model_output": {
      "executive_summary": "The code transforms a CSV file into a Parquet file after applying some cleaning and transformation steps. It reads a CSV, creates a temporary view, performs upper/lower case conversions on 'username' and 'email' columns respectively, cleans the 'username' column using a UDF, prints the resulting rows to the console, and finally writes the cleaned data to a Parquet file.  The main concerns are the lack of error handling, the inefficient use of `collect()` for printing the data and a hardcoded path for reading input. The SparkSession initialization is moved out of the function. The S3 bucket path is removed. The `filter` operation is removed.",
      "syntax_review": {
        "comments": [
          "The code uses a mix of DataFrame operations (withColumn) and SQL-like operations (createOrReplaceTempView, spark.sql). This is acceptable but could be more consistent.",
          "The `my_clean` function is defined within the `main` function, which is acceptable but could be moved outside if it's intended for reuse.",
          "The import statements are correctly placed at the beginning of the file.",
          "The UDF definition and registration are correct.",
          "The `main` function definition and call are correct.",
          "The local file paths are hardcoded. These should be configurable."
        ]
      },
      "code_style": {
        "pep8_compliance": [
          "The code generally follows PEP8 conventions. Spacing and indentation are mostly consistent.",
          "Consider adding docstrings to functions and modules to improve readability and maintainability.",
          "Variable names are generally descriptive.",
          "The use of `time` is not present or active in the current version of the code."
        ],
        "suggestions": [
          "Add a docstring to the `main` function explaining its purpose and arguments.",
          "Consider using more descriptive variable names where appropriate. For example, `data_cleaned` could be more specific based on the transformation applied."
        ]
      },
      "performance_analysis": {
        "spark_specific_concerns": [
          "**`collect()`:**  Using `collect()` to print all rows to the console is extremely inefficient for large datasets. `collect()` brings all the data to the driver node, which can cause memory issues and significantly slow down the application.  This should be removed or replaced with a sampling strategy like `take(n)` or `show()` for debugging purposes only.",
          "**UDF Performance:**  While UDFs provide flexibility, they can be a performance bottleneck in Spark. Because UDFs operate outside of the Spark execution environment, Spark cannot optimize their execution as effectively as built-in functions. Using built-in Spark functions is generally preferred where possible. In this case the `trim` functionality is available. `data_cleaned = data_cleaned.withColumn(\"username\", trim(col(\"username\")))`",
          "**Temporary View:** Using a temporary view is fine for simple cases, but for more complex queries, consider using DataFrame operations directly to avoid potential overhead.",
          "**Hardcoded Paths:** The input and output paths are hardcoded. Using hardcoded paths is not good practice as it makes the script less flexible and harder to deploy in different environments. These should be configurable through command-line arguments or a configuration file.",
          "**Missing Error Handling:**  The code lacks error handling.  Consider adding try-except blocks to handle potential exceptions, such as file not found, invalid data format, etc."
        ],
        "recommendations": [
          "Remove the `collect()` operation and printing to the console. If you need to inspect the data, use `data_cleaned.show()` or `data_cleaned.take(n)` for a small sample.",
          "Replace the UDF with a built-in Spark function like `trim` if possible, which performs the same operation more efficiently.",
          "Externalize the input and output paths into configurable parameters (e.g., using `argparse`).",
          "Add error handling to gracefully handle potential exceptions during file reading, data processing, and writing.",
          "Consider using a logging framework (e.g., `logging`) to log information, warnings, and errors during the ETL process."
        ]
      }
    },
    "error": null
  },
  {
    "name": "teste_marcelo",
    "timestamp": "2025-06-15T11:38:08-07:00",
    "input_type": "file",
    "content": "diff --git a/etl/data_products/consumer/marketing_business_layer/export_trafego_bec.py b/etl/data_products/consumer/marketing_business_layer/export_trafego_bec.py\nnew file mode 100644\nindex 0000000000000000000000000000000000000000..1cf8adcf1712b35fc6a2c247c253c47592a259a7\n--- /dev/null\n+++ b/etl/data_products/consumer/marketing_business_layer/export_trafego_bec.py\n@@ -0,0 +1,1710 @@\n+# Databricks notebook source\n+from ifood_databricks.gcp import gsheet\n+\n+from pyspark.sql import * \n+from pyspark.sql.functions import *\n+from pyspark.sql.types import ArrayType, StructType, StructField, StringType\n+from ifood_databricks import etl, datalake\n+from ifood_databricks.data_products.reader import read, STAGE_GOLD, STAGE_SILVER\n+from ifood_databricks.utils.businessmodelsutils import calculate_business_models\n+from ifood_databricks.toolbelt.data_quality_validator import ifoodDataQualityValidator\n+from datetime import date, timedelta, datetime\n+from ifood_databricks.data_products.reader import read\n+from dateutil.relativedelta import relativedelta\n+\n+from ifood_databricks.data_products.writer import write\n+from ifood_databricks.data_products.config import STAGE_GOLD, STAGE_SILVER\n+from ifood_databricks.utils.businessmodelsutils import calculate_business_models\n+from ifood_databricks.toolbelt.data_quality.dataframe_validator import (\n+    DataQualityDataframeValidator,\n+)\n+from datetime import date, timedelta\n+from ifood_databricks.data_products.reader import read\n+from ifood_data.commons.faster_utils.faster_reader import *\n+from ifood_data.commons.growth_core_utils.monitor import (\n+    notify_execution_start,\n+    notify_execution_end,\n+    notify_custom_message,\n+    create_critical_validation,\n+)\n+import datetime as DT\n+import re\n+from ifood_databricks.gcp import gsheet\n+\n+# COMMAND ----------\n+\n+# MAGIC %md # Par\u00c3\u00a2metros e prepara\u00c3\u00a7\u00c3\u00a3o\n+\n+# COMMAND ----------\n+\n+# MAGIC %md ### Intervalo de datas e leitura de tabelas\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Par\u00c3\u00a2metro de data\n+dia_semana = datetime.today().weekday()\n+\n+# Verifica se \u00c3\u00a9 s\u00c3\u00a1bado (5)\n+if dia_semana >= 5:\n+    date_range_tabelas = 10\n+else:\n+    date_range_tabelas = 3\n+\n+# date_range_tabelas=16\n+date_range_leitura = date_range_tabelas+3 # Ajuste para ler tabelas fonte\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Leitura de tabelas\n+\n+df_00 = read(data_product = 'cities_info', dataset=\"centroid_demographic_segmentation\", options={\"stage\": STAGE_GOLD}) \n+df_00.createOrReplaceTempView('centroid_demographic_segmentation')\n+\n+df_01 = read(data_product = 'consumer_app_sessions', dataset=\"sessions\", options={\"stage\": STAGE_GOLD}) \n+df_01 = df_01.filter(col('dt') >= date_sub(current_date(), date_range_leitura))\n+df_01.createOrReplaceTempView('sessions')\n+\n+df_02 = read(data_product = 'consumer_app_sessions', dataset=\"session_segmentation\", options={\"stage\": STAGE_GOLD}) \n+df_02 = df_02.filter(col('dt') >= date_sub(current_date(), date_range_leitura))\n+df_02.createOrReplaceTempView('session_segmentation')\n+\n+df_03 = read(data_product = 'user_journey', dataset=\"traffic_journey\", options={\"stage\": STAGE_GOLD}) \n+df_03 = df_03.filter(col('dt') >= date_sub(current_date(), date_range_leitura))\n+df_03.createOrReplaceTempView('traffic_journey')\n+\n+df_04 = read(data_product = 'consumer_app_home', dataset=\"home_performance\", options={\"stage\": STAGE_GOLD}) \n+df_04 = df_04.filter(col('dt') >= date_sub(current_date(), date_range_leitura))\n+df_04.createOrReplaceTempView('home_performance')\n+\n+df_05 = read(data_product = 'consumer_app_page', dataset=\"page_performance\", options={\"stage\": STAGE_GOLD}) \n+df_05 = df_05.filter(col('dt') >= date_sub(current_date(), date_range_leitura))\n+df_05.createOrReplaceTempView('page_performance')\n+\n+\n+df_06 = read(data_product = 'cardstack_cms', dataset=\"collection_changes\", options={\"stage\": STAGE_GOLD}) \n+df_06.createOrReplaceTempView('collection_changes')\n+\n+df_07 = read(data_product = 'cardstack_cms', dataset=\"card_contents\", options={\"stage\": STAGE_GOLD}) \n+df_07.createOrReplaceTempView('card_contents')\n+\n+df_08 = read(data_product = 'cardstack_cms', dataset=\"page_cards\", options={\"stage\": STAGE_GOLD}) \n+df_08.createOrReplaceTempView('page_cards')\n+\n+df_09 = read(data_product = 'cms_summary', dataset=\"unique_per_day\", options={\"stage\": STAGE_GOLD}) \n+df_09.createOrReplaceTempView('unique_per_day')\n+\n+\n+df_10 = read(data_product = 'consumer_app_search', dataset=\"search_summary\", options={\"stage\": STAGE_GOLD}) \n+df_10 = df_10.filter(col('dt') >= date_sub(current_date(), date_range_leitura))\n+df_10.createOrReplaceTempView('search_summary')\n+\n+df_11 = read(data_product = 'faster_remote_config', dataset=\"schema\", options={\"stage\": STAGE_GOLD}) \n+df_11.createOrReplaceTempView('faster_schema')\n+\n+df_12 = read(data_product = 'crm_channel_analytics', dataset=\"inapp_account\", options={\"stage\": STAGE_GOLD}) \n+df_12 = df_12.filter(col('event_date') >= date_sub(current_date(), date_range_leitura))\n+df_12.createOrReplaceTempView('inapp_account')\n+\n+\n+df_13 = read(data_product = 'marketing_business_layer', dataset=\"trafego_bec\", options={\"stage\": STAGE_GOLD}) \n+# df_13 = df_13.filter(col('dt_amsp') >= date_sub(current_date(), date_range_leitura))\n+df_13.createOrReplaceTempView('trafego_bec_historico')\n+\n+df_14 = read(data_product = 'marketing_analytics_layer', dataset=\"orders_order_metrics\", options={\"stage\": STAGE_GOLD}) \n+df_14 = df_14.filter(col('order_reference_date_local') >= date_sub(current_date(), date_range_leitura+5))\n+df_14.createOrReplaceTempView('orders_order_metrics')\n+\n+\n+\n+\n+faster_click_dynamic_content = (event_read(\n+      product = 'ifood',\n+      event_name = 'click_dynamic_content',\n+      start_date = date_sub(current_date(), date_range_tabelas),\n+      end_date = current_date()\n+    ))\n+faster_click_dynamic_content.createOrReplaceTempView('faster_click_dynamic_content')\n+\n+faster_app_received_deeplink = (event_read(\n+      product = 'ifood',\n+      event_name = 'app_received_deeplink',\n+      start_date = date_sub(current_date(), date_range_tabelas),\n+      end_date = current_date()\n+    ))\n+faster_app_received_deeplink.createOrReplaceTempView('faster_app_received_deeplink')\n+\n+faster_app_push_opened = (event_read(\n+      product = 'ifood',\n+      event_name = 'app_push_opened',\n+      start_date = date_sub(current_date(), date_range_tabelas),\n+      end_date = current_date()\n+    ))\n+faster_app_push_opened.createOrReplaceTempView('faster_app_push_opened')\n+\n+faster_click_offer_grocery_component = (event_read(\n+      product = 'ifood',\n+      event_name = 'click_offer_grocery_component',\n+      start_date = date_sub(current_date(), date_range_tabelas),\n+      end_date = current_date()\n+    ))\n+faster_click_offer_grocery_component.createOrReplaceTempView('faster_click_offer_grocery_component')\n+\n+faster_click_perform_search = (event_read(\n+      product = 'ifood',\n+      event_name = 'click_perform_search',\n+      start_date = date_sub(current_date(), date_range_tabelas),\n+      end_date = current_date()\n+    ))\n+faster_click_perform_search.createOrReplaceTempView('faster_click_perform_search')\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Datas de corte\n+params_df = spark.createDataFrame([(1,)], [\"dummy\"])\n+\n+params_df = params_df.withColumn(\"dt_ref_amsp\", date_sub(current_date(), date_range_tabelas)) \\\n+                     .withColumn(\"dt_ref\", date_sub(current_date(), 2+date_range_tabelas)) \\\n+                     .withColumn(\"dt_ref_fim\", date_sub(current_date(),-1)) \\\n+                     .drop(\"dummy\")\n+\n+params_df.createOrReplaceTempView(\"params\")\n+\n+\n+# COMMAND ----------\n+\n+# MAGIC %md ### Dados do projeto\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Merchants do projeto\n+# MAGIC %sql\n+# MAGIC create or replace temp view merchants_projeto as\n+# MAGIC\n+# MAGIC select distinct m.name as nome, m.merchant_id, m.short_id as frn_id, to_date(m.activated_at) as dt_ativacao\n+# MAGIC from main.alexandria.codex_cinc_gcor_beverage_n_convenience_queries_bec_merchants() m\n+# MAGIC\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Listas do projeto\n+# MAGIC %sql\n+# MAGIC create or replace temp view listas_projeto as\n+# MAGIC\n+# MAGIC select * from main.alexandria.codex_cinc_gcor_beverage_n_convenience_queries_bec_lists()\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,P\u00c3\u00a1ginas do projeto\n+# MAGIC %sql\n+# MAGIC create or replace temp view paginas_projeto as\n+# MAGIC\n+# MAGIC select * from main.alexandria.codex_cinc_gcor_beverage_n_convenience_queries_bec_pages()\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Cidades do projeto\n+# MAGIC %sql\n+# MAGIC create or replace temp view cidades_projeto as\n+# MAGIC\n+# MAGIC select * from main.alexandria.codex_cinc_gcor_beverage_n_convenience_queries_bec_cities(0)\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Homes B&C do projeto\n+# MAGIC %sql\n+# MAGIC create or replace temp view homes_projeto as\n+# MAGIC\n+# MAGIC select * from main.alexandria.codex_cinc_gcor_beverage_n_convenience_queries_bec_homes_ref()\n+# MAGIC where tipo_home in ('nova','nova_teste','query_nova')\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Paginas e Paginas de Listas do projeto\n+# MAGIC %sql\n+# MAGIC create or replace temp view paginas_totais_projeto as\n+# MAGIC\n+# MAGIC select distinct collection_page_id as page_id\n+# MAGIC\n+# MAGIC from collection_changes\n+# MAGIC where collection_id in (select collection_id from listas_projeto)\n+# MAGIC\n+# MAGIC union\n+# MAGIC\n+# MAGIC select distinct page_id from paginas_projeto\n+\n+# COMMAND ----------\n+\n+# MAGIC %md # Fontes de acesso\n+\n+# COMMAND ----------\n+\n+# MAGIC %md ## Cards e Contents q d\u00c3\u00a3o acesso \u00c3 s paginas\n+# MAGIC _Banners, Grid, Carrossel, Pre Busca, Waiting_\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Contents q levam pras paginas\n+# MAGIC %sql\n+# MAGIC create or replace temp view cards_contents_pages_bec_0 as\n+# MAGIC\n+# MAGIC ----------------------------------- CONTENTS E SUAS P\u00c3\u0081GINAS DE DESTINO QUANDO CLICADOS -----------------------------------\n+# MAGIC\n+# MAGIC with cards_contents_pages_geral as (\n+# MAGIC\n+# MAGIC select distinct card_id, content_id, collection_id, content_action.type, content_action.value,\n+# MAGIC\n+# MAGIC case when content_action.value like '%93147d52-89aa-4bf6-9239-4f122edf4dc2%' then '93147d52-89aa-4bf6-9239-4f122edf4dc2' \n+# MAGIC when content_action.value like '%page?identifier=%' then regexp_extract(content_action.value, 'identifier=([a-zA-Z0-9\\\\-]+)', 1) \n+# MAGIC when content_action.type='page' then regexp_extract(content_action.value, 'identifier=([a-zA-Z0-9\\\\-]+)', 1) \n+# MAGIC when content_action.type='custom' then regexp_extract(content_action.value, 'type=([a-zA-Z0-9_\\\\-]+)', 1)\n+# MAGIC when content_action.type='collection' then content_action.value else content_action.value end as page_id_destino\n+# MAGIC\n+# MAGIC from card_contents\n+# MAGIC -- where content_id in ('c6145741-3fa9-402d-a52c-ff4edcaf19c5','c36f5946-3981-463a-b213-af5a1240115c','a715b939-130d-43ee-93bf-087e577999a6','c437edcd-d61e-498c-812a-858729dcf930', 'f409e616-bd83-481b-be23-4c86e4d60100','49263ebb-5ec6-4fe1-8975-08068ac99948')\n+# MAGIC -- or card_id='af493471-9489-42a6-95ac-b965f6b7749e'\n+# MAGIC\n+# MAGIC ----------------------------------- P\u00c3\u0081GINAS Q CORRESPONDEM \u00c3\u20ac JORNADA NOVA DE BEBIDAS -----------------------------------\n+# MAGIC\n+# MAGIC ), pages as (\n+# MAGIC\n+# MAGIC select * from paginas_totais_projeto\n+# MAGIC union\n+# MAGIC select 'LIQUORSTORE' as page_id\n+# MAGIC\n+# MAGIC\n+# MAGIC ----------------------------------- CLASSIFICAR AS PAGINAS DE DESTINO ENTRE HOME B&C OU OUTRAS -----------------------------------\n+# MAGIC\n+# MAGIC ), cms_home_bec as (\n+# MAGIC\n+# MAGIC select home_id, home_title from homes_projeto\n+# MAGIC\n+# MAGIC -- select id as home_id, name as home_title\n+# MAGIC -- from unique_per_day\n+# MAGIC -- where level='page' and validity_end_date='9999-12-31'\n+# MAGIC -- and type.home_title_grouped='Bebidas'\n+# MAGIC -- and name NOT RLIKE '^[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}$'\n+# MAGIC\n+# MAGIC ----------------------------------- PEGAR NOME DO CARD -----------------------------------\n+# MAGIC\n+# MAGIC ), cms_card_name as (\n+# MAGIC\n+# MAGIC select id as card_id_cms, name as card_name, type.component_type\n+# MAGIC from unique_per_day\n+# MAGIC where level='card' and validity_end_date='9999-12-31'\n+# MAGIC\n+# MAGIC ----------------------------------- PEGAR NOME DO CONTENT -----------------------------------\n+# MAGIC\n+# MAGIC ), cms_content_name as (\n+# MAGIC\n+# MAGIC select id as content_id_cms, name as content_name\n+# MAGIC from unique_per_day\n+# MAGIC where level='content' and validity_end_date='9999-12-31'\n+# MAGIC\n+# MAGIC\n+# MAGIC ----------------------------------- PEGAR CLASSIFICA\u00c3\u2021\u00c3\u0192O DA P\u00c3\u0081GINA DOS CARDS -----------------------------------\n+# MAGIC\n+# MAGIC ), cms_pages_type as (\n+# MAGIC\n+# MAGIC select id as page_id_cms, name as page_name_2, type.page_name, type.home_title_grouped, type.page_type, type.page_category,\n+# MAGIC\n+# MAGIC case \n+# MAGIC when type.home_title_grouped in ('Bebidas','Express','Farmacia','Ifood Economico','Inicio','Mercado','Pet','Restaurante','Shopping') then concat('Home - ',type.home_title_grouped)\n+# MAGIC when type.page_type='SUBHOME' then 'Page'\n+# MAGIC when type.page_type='SEARCH' then 'Pre Busca'\n+# MAGIC when type.page_type='WAITING' then 'Waiting'\n+# MAGIC else lower(type.page_type) end as tipo_pagina_origem\n+# MAGIC\n+# MAGIC from unique_per_day\n+# MAGIC where level='page' and validity_end_date='9999-12-31'\n+# MAGIC\n+# MAGIC ----------------------------------- VER ONDE OS CARDS EST\u00c3\u0192O NO APP -----------------------------------\n+# MAGIC\n+# MAGIC ), page_cards_cms_0 as (\n+# MAGIC\n+# MAGIC select distinct a.card_id, b.tipo_pagina_origem, count(distinct a.page_id) as paginas_presente\n+# MAGIC\n+# MAGIC from page_cards a\n+# MAGIC inner join cms_pages_type b\n+# MAGIC on a.page_id=b.page_id_cms\n+# MAGIC group by all\n+# MAGIC\n+# MAGIC ), page_type_cards as (\n+# MAGIC\n+# MAGIC select card_id as card_id_page_type, tipo_pagina_origem\n+# MAGIC\n+# MAGIC from page_cards_cms_0\n+# MAGIC qualify row_number() over (partition by card_id order by paginas_presente desc)=1\n+# MAGIC\n+# MAGIC ----------------------------------- CLASSIFICA\u00c3\u2021\u00c3\u0192O FINAL -----------------------------------\n+# MAGIC\n+# MAGIC ), cards_contents_pages_bec_0 as (\n+# MAGIC\n+# MAGIC select card_id, card_name, component_type, content_id, content_name, collection_id,  page_id_destino, tipo_pagina_origem,\n+# MAGIC -- case when page_id_destino='LIQUORSTORE' then '93147d52-89aa-4bf6-9239-4f122edf4dc2' else page_id_destino end as page_id_destino,\n+# MAGIC\n+# MAGIC type='collection' as card_ver_mais, type, value,\n+# MAGIC\n+# MAGIC case when cms.home_id is not null or page_id_destino='LIQUORSTORE' then 'Home B&C' else 'Subhome/Lista B&C' end as tipo_pagina_destino,\n+# MAGIC case when cms.home_id is not null or page_id_destino='LIQUORSTORE' then 'Home B&C' else page_id_destino end as criterio_join_acesso\n+# MAGIC\n+# MAGIC\n+# MAGIC from cards_contents_pages_geral a\n+# MAGIC inner join pages b\n+# MAGIC on a.page_id_destino=b.page_id\n+# MAGIC left join cms_home_bec cms\n+# MAGIC on a.page_id_destino=cms.home_id\n+# MAGIC left join cms_card_name cards\n+# MAGIC on a.card_id=cards.card_id_cms\n+# MAGIC left join page_type_cards pages\n+# MAGIC on a.card_id=pages.card_id_page_type\n+# MAGIC left join cms_content_name contents\n+# MAGIC on a.content_id=contents.content_id_cms\n+# MAGIC\n+# MAGIC )\n+# MAGIC\n+# MAGIC select *, row_number() over (partition by card_id, content_id, criterio_join_acesso order by content_id desc) as rn_card_content\n+# MAGIC from cards_contents_pages_bec_0\n+# MAGIC\n+# MAGIC\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Temp view contents\n+df_cards_contents_pages_bec_0 = spark.sql(\"\"\"SELECT * FROM cards_contents_pages_bec_0\"\"\")\n+\n+validated_df_cards_contents_pages_bec_0 = (\n+  datalake.dataframe2tempdataset(\n+    dataframe=df_cards_contents_pages_bec_0, \n+    namespace=\"growth\", \n+    dataset=f\"cards_contents_pages_bec_0{str(DT.datetime.now())}\", \n+    force_s3=True\n+  )\n+)\n+\n+validated_df_cards_contents_pages_bec_0.createOrReplaceTempView(\"cards_contents_pages_bec\")\n+\n+# COMMAND ----------\n+\n+# MAGIC %md ### Clicks nos cards e contents\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Click_dynamic_content\n+# MAGIC %sql\n+# MAGIC create or replace temp view clicks_page_bec_0 as \n+# MAGIC\n+# MAGIC with page_ids as ( \n+# MAGIC\n+# MAGIC select * from paginas_totais_projeto\n+# MAGIC\n+# MAGIC ------------------------- PEGAR SESS\u00c3\u2022ES DE PAGINAS ------------------------- \n+# MAGIC\n+# MAGIC ), trafego_page_0 as (\n+# MAGIC\n+# MAGIC select /*+ BROADCAST(params) */\n+# MAGIC distinct a.session_id\n+# MAGIC\n+# MAGIC from page_performance a\n+# MAGIC join params\n+# MAGIC inner join page_ids h\n+# MAGIC on a.page_id=h.page_id\n+# MAGIC\n+# MAGIC where a.dt between dt_ref and dt_ref_fim\n+# MAGIC\n+# MAGIC ------------------------- CLIQUES DO FASTER J\u00c3\u0081 FILTRADOS PARA SESS\u00c3\u2022ES DE BEBIDAS ------------------------- \n+# MAGIC\n+# MAGIC ), faster_click_dyn as (\n+# MAGIC\n+# MAGIC select /*+ BROADCAST(params) */\n+# MAGIC dt, a.session_id, `timestamp` as dia_hora, content_id as content_id_faster, card_id as card_id_faster, feature as feature_faster\n+# MAGIC\n+# MAGIC from faster_click_dynamic_content a\n+# MAGIC join params\n+# MAGIC inner join trafego_page_0 b\n+# MAGIC on a.session_id=b.session_id\n+# MAGIC\n+# MAGIC where dt between dt_ref and dt_ref_fim\n+# MAGIC and name='click_dynamic_content'\n+# MAGIC\n+# MAGIC ------------------------- JOIN COM OS CARDS DE BEBIDAS ------------------------- \n+# MAGIC\n+# MAGIC ), clicks_cards_contents as (\n+# MAGIC\n+# MAGIC select /*+ BROADCAST(params, cards_contents_pages_bec) */\n+# MAGIC dt, session_id, dia_hora, content_id_faster, card_id_faster, b.*\n+# MAGIC\n+# MAGIC from faster_click_dyn a\n+# MAGIC join params\n+# MAGIC inner join cards_contents_pages_bec b\n+# MAGIC on a.card_id_faster=b.card_id and a.content_id_faster=b.content_id and b.rn_card_content=1 and card_ver_mais is false\n+# MAGIC\n+# MAGIC union all\n+# MAGIC\n+# MAGIC select /*+ BROADCAST(params, cards_contents_pages_bec) */\n+# MAGIC dt, session_id, dia_hora, content_id_faster, card_id_faster, b.*\n+# MAGIC\n+# MAGIC from faster_click_dyn a\n+# MAGIC join params\n+# MAGIC inner join cards_contents_pages_bec b\n+# MAGIC on a.card_id_faster=b.card_id and b.rn_card_content=1 and card_ver_mais is true\n+# MAGIC\n+# MAGIC where true\n+# MAGIC and feature_faster in ('Home - see more','Discovery Page - see more')\n+# MAGIC\n+# MAGIC )\n+# MAGIC\n+# MAGIC select * from clicks_cards_contents\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Temp view Click_dynamic_content\n+df_clicks_page_bec_0 = spark.sql(\"\"\"SELECT * FROM clicks_page_bec_0\"\"\")\n+\n+validated_df_clicks_page_bec_0 = (\n+  datalake.dataframe2tempdataset(\n+    dataframe=df_clicks_page_bec_0, \n+    namespace=\"growth\", \n+    dataset=f\"clicks_page_bec_0{str(DT.datetime.now())}\", \n+    force_s3=True\n+  )\n+)\n+\n+validated_df_clicks_page_bec_0.createOrReplaceTempView(\"clicks_page_bec\")\n+\n+# COMMAND ----------\n+\n+# MAGIC %md ## Outras fontes de acesso \u00c3 s p\u00c3\u00a1ginas\n+# MAGIC _Deeplink, Push, InApp, Busca_\n+\n+# COMMAND ----------\n+\n+# MAGIC %md ### Busca\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Termos Buscados - Faster Bebidas\n+# import re\n+\n+# Passo 1: obter a data mais recente com schema_name desejado\n+latest_dt = (\n+    spark.table(\"faster_schema\")  \n+    .filter(col(\"schema_name\") == \"search_pre_indexed_results_list\")\n+    .agg(expr(\"max(dt)\").alias(\"max_dt\"))\n+    .collect()[0][\"max_dt\"]\n+)\n+\n+# Passo 2: filtrar o dataset com essa data e schema_name\n+df = (\n+    spark.table(\"faster_schema\")\n+    .filter(\n+        (col(\"schema_name\") == \"search_pre_indexed_results_list\") &\n+        (col(\"dt\") == lit(latest_dt))\n+    )\n+)\n+\n+# Passo 3: explode o array de conditions\n+exploded = df.selectExpr(\"explode(conditions) as cond\")\n+\n+# Passo 4: filtra apenas os que t\u00c3\u00aam \"Bebidas\" no nome\n+bebidas = exploded.filter(col(\"cond.name\").contains(\"Bebidas\"))\n+\n+# Passo 5: fun\u00c3\u00a7\u00c3\u00a3o para extrair UUIDs e termos\n+def extract_uuid_terms(value_raw):\n+    if not value_raw:\n+        return []\n+    try:\n+        # regex para pegar todos os blocos: action + terms\n+        pattern = r'page\\?identifier=([a-f0-9\\-]{36}).*?terms\": \\[(.*?)\\]'\n+        matches = re.findall(pattern, value_raw, re.DOTALL)\n+        result = []\n+        for uuid, terms_raw in matches:\n+            # limpa e separa os termos\n+            terms = re.findall(r'\"([^\"]+)\"', terms_raw)\n+            result.append({\"uuid\": uuid, \"terms\": terms})\n+        return result\n+    except Exception as e:\n+        return []\n+\n+# Passo 6: registrar UDF\n+extract_udf = udf(extract_uuid_terms, ArrayType(StructType([\n+    StructField(\"uuid\", StringType()),\n+    StructField(\"terms\", ArrayType(StringType()))\n+])))\n+\n+# Passo 7: aplicar a UDF no campo cond.values.value\n+parsed = bebidas.withColumn(\"parsed\", extract_udf(col(\"cond.values.value\")))\n+\n+# Passo 8: explode os resultados\n+exploded_uuids = parsed.select(\n+    col(\"cond.name\").alias(\"condition_name\"),\n+    explode(col(\"parsed\")).alias(\"entry\")\n+).select(\n+    \"condition_name\",\n+    col(\"entry.uuid\").alias(\"page_id_destino\"),\n+    explode(col(\"entry.terms\")).alias(\"term\")\n+)\n+\n+# Resultado final\n+# exploded_uuids.show(truncate=False)\n+display(exploded_uuids)\n+exploded_uuids.createOrReplaceTempView(\"termos_buscados\")\n+\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Clicks Busca\n+# MAGIC %sql\n+# MAGIC create or replace temp view clicks_termos_busca_0 as\n+# MAGIC\n+# MAGIC with termos_historico as (\n+# MAGIC\n+# MAGIC select distinct lower(term) as term, page_id_destino from termos_buscados \n+# MAGIC union \n+# MAGIC select distinct lower(entry_point_details.content_name) as term, acesso_bec_details.page_id as page_id_destino\n+# MAGIC from trafego_bec_historico\n+# MAGIC where entry_point='Busca' and query_bec='query_page'\n+# MAGIC\n+# MAGIC ), fonte_faster as (\n+# MAGIC\n+# MAGIC select /*+ BROADCAST(params) */\n+# MAGIC a.session_id, (a.timestamp) as dia_hora, a.term as search_term, page_id_destino, \n+# MAGIC case when cms.home_id is not null or page_id_destino='LIQUORSTORE' then 'Home B&C' else page_id_destino end as criterio_join_acesso\n+# MAGIC\n+# MAGIC from faster_click_perform_search a\n+# MAGIC join params\n+# MAGIC inner join termos_historico b\n+# MAGIC on lower(a.term)=lower(b.term)\n+# MAGIC left join homes_projeto cms\n+# MAGIC on b.page_id_destino=cms.home_id\n+# MAGIC\n+# MAGIC where dt>=dt_ref\n+# MAGIC\n+# MAGIC ), fonte_summary as (\n+# MAGIC\n+# MAGIC select /*+ BROADCAST(params) */\n+# MAGIC session_id, (first_search_at) as dia_hora, search_term, page_id_destino, \n+# MAGIC case when cms.home_id is not null or page_id_destino='LIQUORSTORE' then 'Home B&C' else page_id_destino end as criterio_join_acesso\n+# MAGIC\n+# MAGIC from search_summary a\n+# MAGIC join params\n+# MAGIC inner join termos_historico b\n+# MAGIC on lower(a.search_term)=lower(b.term)\n+# MAGIC left join homes_projeto cms\n+# MAGIC on b.page_id_destino=cms.home_id\n+# MAGIC\n+# MAGIC where dt>=dt_ref\n+# MAGIC\n+# MAGIC )\n+# MAGIC\n+# MAGIC select * from fonte_faster\n+# MAGIC union all\n+# MAGIC select * from fonte_summary\n+# MAGIC\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Temp view Busca\n+df_clicks_termos_busca_0 = spark.sql(\"\"\"SELECT * FROM clicks_termos_busca_0\"\"\")\n+\n+validated_df_clicks_termos_busca_0 = (\n+  datalake.dataframe2tempdataset(\n+    dataframe=df_clicks_termos_busca_0, \n+    namespace=\"growth\", \n+    dataset=f\"clicks_termos_busca_0{str(DT.datetime.now())}\", \n+    force_s3=True\n+  )\n+)\n+\n+validated_df_clicks_termos_busca_0.createOrReplaceTempView(\"clicks_termos_busca\")\n+\n+# COMMAND ----------\n+\n+# MAGIC %md ### InApp + Push\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,InApp\n+# MAGIC %sql\n+# MAGIC create or replace temp view clicks_inapps_0 as\n+# MAGIC\n+# MAGIC with inapp_sessions as (\n+# MAGIC\n+# MAGIC select /*+ BROADCAST(params) */\n+# MAGIC distinct session_id[0] as session_id, campaign_name, campaign_id\n+# MAGIC\n+# MAGIC from inapp_account \n+# MAGIC join params d\n+# MAGIC\n+# MAGIC where event_date>=dt_ref\n+# MAGIC and campaign_name rlike 'CRM BEC|CRM_GRC / INAPP / BEC'\n+# MAGIC and sample_type<>'control'\n+# MAGIC and inapp_count.click_count>0\n+# MAGIC and session_id[0] is not null\n+# MAGIC\n+# MAGIC ), faster_events_inapp as (\n+# MAGIC\n+# MAGIC select /*+ BROADCAST(params) */\n+# MAGIC dt, a.session_id, `timestamp` as dia_hora, \n+# MAGIC 'Inapp' as ponto_acesso, campaign_name, campaign_id,\n+# MAGIC regexp_extract(url, '([a-f0-9\\\\-]{36})(?!.*[a-f0-9\\\\-]{36})', 1) AS page_id_destino\n+# MAGIC\n+# MAGIC from faster_app_received_deeplink a\n+# MAGIC join params d\n+# MAGIC inner join inapp_sessions b on a.session_id=b.session_id\n+# MAGIC where dt>=dt_ref\n+# MAGIC and name='app_received_deeplink'\n+# MAGIC\n+# MAGIC )\n+# MAGIC\n+# MAGIC select session_id, dia_hora, case when cms.home_id is not null or page_id_destino='LIQUORSTORE' then 'Home B&C' else page_id_destino end as criterio_join_acesso,\n+# MAGIC\n+# MAGIC 'InApp' as entry_point, \n+# MAGIC 'InApp' as entry_point_details_component_type,\n+# MAGIC\n+# MAGIC campaign_name as entry_point_details_card_name,\n+# MAGIC campaign_id as entry_point_details_card_id,\n+# MAGIC campaign_name as entry_point_details_content_name,\n+# MAGIC campaign_id as entry_point_details_content_id\n+# MAGIC\n+# MAGIC from faster_events_inapp a\n+# MAGIC left join homes_projeto cms\n+# MAGIC on a.page_id_destino=cms.home_id\n+# MAGIC\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Temp view InApp\n+df_clicks_inapps_0 = spark.sql(\"\"\"SELECT * FROM clicks_inapps_0\"\"\")\n+\n+validated_df_clicks_inapps_0 = (\n+  datalake.dataframe2tempdataset(\n+    dataframe=df_clicks_inapps_0, \n+    namespace=\"growth\", \n+    dataset=f\"clicks_inapps_0{str(DT.datetime.now())}\", \n+    force_s3=True\n+  )\n+)\n+\n+validated_df_clicks_inapps_0.createOrReplaceTempView(\"clicks_inapps\")\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Push\n+# MAGIC %sql\n+# MAGIC create or replace temp view clicks_pushes_0 as\n+# MAGIC\n+# MAGIC with faster_events_push as (\n+# MAGIC\n+# MAGIC select /*+ BROADCAST(params) */\n+# MAGIC dt, a.session_id, `timestamp` as dia_hora, \n+# MAGIC 'Push' as ponto_acesso,\n+# MAGIC push_external_id as content_id,\n+# MAGIC title as content_name,\n+# MAGIC deeplink as deeplink,\n+# MAGIC regexp_extract(deeplink, '([a-f0-9\\\\-]{36})(?!.*[a-f0-9\\\\-]{36})', 1) AS page_id_destino\n+# MAGIC\n+# MAGIC from faster_app_push_opened a\n+# MAGIC join params d\n+# MAGIC where dt>=dt_ref\n+# MAGIC and name='app_push_opened'\n+# MAGIC and push_external_id rlike 'CRM_GRC_GROC_BEC|BEBIDAS / SOROCABA E VOTORANTIM'\n+# MAGIC\n+# MAGIC )\n+# MAGIC\n+# MAGIC select session_id, dia_hora, case when cms.home_id is not null or page_id_destino='LIQUORSTORE' then 'Home B&C' else page_id_destino end as criterio_join_acesso,\n+# MAGIC\n+# MAGIC 'Push' as entry_point, \n+# MAGIC 'Push' as entry_point_details_component_type,\n+# MAGIC\n+# MAGIC content_name as entry_point_details_card_name,\n+# MAGIC deeplink as entry_point_details_card_id,\n+# MAGIC content_name as entry_point_details_content_name,\n+# MAGIC content_id as entry_point_details_content_id\n+# MAGIC\n+# MAGIC from faster_events_push a\n+# MAGIC left join homes_projeto cms\n+# MAGIC on a.page_id_destino=cms.home_id\n+# MAGIC\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Temp view Push\n+df_clicks_pushes_0 = spark.sql(\"\"\"SELECT * FROM clicks_pushes_0\"\"\")\n+\n+validated_df_clicks_pushes_0 = (\n+  datalake.dataframe2tempdataset(\n+    dataframe=df_clicks_pushes_0, \n+    namespace=\"growth\", \n+    dataset=f\"clicks_pushes_0{str(DT.datetime.now())}\", \n+    force_s3=True\n+  )\n+)\n+\n+validated_df_clicks_pushes_0.createOrReplaceTempView(\"clicks_pushes\")\n+\n+# COMMAND ----------\n+\n+# MAGIC %md ### Outros\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Footer Especial\n+# MAGIC %sql\n+# MAGIC create or replace temp view clicks_footer_especial_0 as\n+# MAGIC\n+# MAGIC with faster_events_footer_especial as (\n+# MAGIC\n+# MAGIC select /*+ BROADCAST(params) */\n+# MAGIC dt, a.session_id, `timestamp` as dia_hora, \n+# MAGIC\n+# MAGIC name as name_event,\n+# MAGIC a.page_id,\n+# MAGIC subhome_page_id as page_id_destino\n+# MAGIC\n+# MAGIC from faster_click_offer_grocery_component a\n+# MAGIC join params d\n+# MAGIC\n+# MAGIC inner join paginas_totais_projeto b\n+# MAGIC on a.subhome_page_id=b.page_id\n+# MAGIC\n+# MAGIC where dt>=dt_ref\n+# MAGIC and name='click_offer_grocery_component'\n+# MAGIC\n+# MAGIC )\n+# MAGIC\n+# MAGIC select session_id, dia_hora, case when cms.home_id is not null or page_id_destino='LIQUORSTORE' then 'Home B&C' else page_id_destino end as criterio_join_acesso,\n+# MAGIC\n+# MAGIC 'Waiting' as entry_point, \n+# MAGIC 'Footer inicial' as entry_point_details_component_type,\n+# MAGIC\n+# MAGIC 'Footer inicial' as entry_point_details_card_name,\n+# MAGIC a.page_id as entry_point_details_card_id,\n+# MAGIC a.name_event as entry_point_details_content_name,\n+# MAGIC a.page_id as entry_point_details_content_id\n+# MAGIC\n+# MAGIC from faster_events_footer_especial a\n+# MAGIC left join homes_projeto cms\n+# MAGIC on a.page_id_destino=cms.home_id\n+# MAGIC\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Temp view Footer Especial\n+df_clicks_footer_especial_0 = spark.sql(\"\"\"SELECT * FROM clicks_footer_especial_0\"\"\")\n+\n+validated_df_clicks_footer_especial_0 = (\n+  datalake.dataframe2tempdataset(\n+    dataframe=df_clicks_footer_especial_0, \n+    namespace=\"growth\", \n+    dataset=f\"clicks_footer_especial_0{str(DT.datetime.now())}\", \n+    force_s3=True\n+  )\n+)\n+\n+validated_df_clicks_footer_especial_0.createOrReplaceTempView(\"clicks_footer_especial\")\n+\n+# COMMAND ----------\n+\n+# MAGIC %md ## Fontes de acesso final\n+# MAGIC _Union das tabelas clicks+busca_\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Union Clicks\n+# MAGIC %sql\n+# MAGIC create or replace temp view union_clicks as\n+# MAGIC\n+# MAGIC select session_id, dia_hora, criterio_join_acesso, \n+# MAGIC tipo_pagina_origem as entry_point, \n+# MAGIC component_type as entry_point_details_component_type,\n+# MAGIC card_name as entry_point_details_card_name,\n+# MAGIC card_id as entry_point_details_card_id,\n+# MAGIC content_name as entry_point_details_content_name,\n+# MAGIC content_id as entry_point_details_content_id\n+# MAGIC\n+# MAGIC from clicks_page_bec\n+# MAGIC\n+# MAGIC union all\n+# MAGIC\n+# MAGIC select session_id, dia_hora, criterio_join_acesso, \n+# MAGIC 'Busca' as entry_point, \n+# MAGIC 'Busca' as entry_point_details_component_type,\n+# MAGIC search_term as entry_point_details_card_name,\n+# MAGIC null as entry_point_details_card_id,\n+# MAGIC search_term as entry_point_details_content_name,\n+# MAGIC null as entry_point_details_content_id\n+# MAGIC\n+# MAGIC from clicks_termos_busca\n+# MAGIC\n+# MAGIC union all\n+# MAGIC\n+# MAGIC select session_id, dia_hora, criterio_join_acesso, \n+# MAGIC entry_point, \n+# MAGIC entry_point_details_component_type,\n+# MAGIC entry_point_details_card_name, entry_point_details_card_id,\n+# MAGIC entry_point_details_content_name, entry_point_details_content_id\n+# MAGIC\n+# MAGIC from clicks_pushes\n+# MAGIC\n+# MAGIC union all\n+# MAGIC\n+# MAGIC select session_id, dia_hora, criterio_join_acesso, \n+# MAGIC entry_point, \n+# MAGIC entry_point_details_component_type,\n+# MAGIC entry_point_details_card_name, entry_point_details_card_id,\n+# MAGIC entry_point_details_content_name, entry_point_details_content_id\n+# MAGIC\n+# MAGIC from clicks_inapps\n+# MAGIC\n+# MAGIC union all\n+# MAGIC\n+# MAGIC select session_id, dia_hora, criterio_join_acesso, \n+# MAGIC entry_point, \n+# MAGIC entry_point_details_component_type,\n+# MAGIC entry_point_details_card_name, entry_point_details_card_id,\n+# MAGIC entry_point_details_content_name, entry_point_details_content_id\n+# MAGIC\n+# MAGIC from clicks_footer_especial\n+\n+# COMMAND ----------\n+\n+# MAGIC %md # Tr\u00c3\u00a1fego B&C\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Sess\u00c3\u00b5es app + localiza\u00c3\u00a7\u00c3\u00a3o\n+# MAGIC %sql\n+# MAGIC create or replace temp view sessoes_app as \n+# MAGIC with centroids as (\n+# MAGIC\n+# MAGIC select distinct centroid_id, city_state_attributes.state, city_state_attributes.city \n+# MAGIC from centroid_demographic_segmentation\n+# MAGIC qualify row_number() over (partition by centroid_id order by centroid_id) = 1\n+# MAGIC\n+# MAGIC ), sessions_ajustada as (\n+# MAGIC\n+# MAGIC select session_id, account_id, dt_amsp, dt, started_at, started_at_amsp,\n+# MAGIC centroid_id, latitude, longitude, concat(round(latitude,2),round(longitude,2)) as centroid_id_ajustado, city, state\n+# MAGIC\n+# MAGIC from sessions b\n+# MAGIC join params\n+# MAGIC\n+# MAGIC where dt between dt_ref and dt_ref_fim\n+# MAGIC and dt_amsp>=dt_ref_amsp\n+# MAGIC\n+# MAGIC ), lifecycle as (\n+# MAGIC\n+# MAGIC select session_id,\n+# MAGIC\n+# MAGIC case\n+# MAGIC when user_info.groceries_beverage_lifecycle_v2 in ('active','cold_active','cold_engaged','hot_active','warm_active','warm_engaged','warm_reengaged','cold_reengaged','engaged','hot_engaged','hot_reengaged','reengaged','hot_new','hot_reactivated') then 'active'\n+# MAGIC when user_info.groceries_beverage_lifecycle_v2 in ('early_churn','late_churn','lost_churn','middle_churn') then 'churn'\n+# MAGIC when user_info.groceries_beverage_lifecycle_v2 in ('prospect') then 'prospect'\n+# MAGIC when user_info.groceries_beverage_lifecycle_v2 in ('new') then 'new_m0'\n+# MAGIC when user_info.groceries_beverage_lifecycle_v2 in ('reactivated') then 'reactivated_m0'\n+# MAGIC when user_info.groceries_beverage_lifecycle_v2 in ('cold_new','warm_new') then 'retention_m1_new'\n+# MAGIC when user_info.groceries_beverage_lifecycle_v2 in ('cold_reactivated','warm_reactivated') then 'retention_m1_reactivated'\n+# MAGIC else coalesce(user_info.groceries_beverage_lifecycle_v2,'Unclassified') end as lifecycle_bec,\n+# MAGIC\n+# MAGIC case\n+# MAGIC when user_info.ifood_lifecycle_v2 in ('active','cold_active','cold_engaged','hot_active','warm_active','warm_engaged','warm_reengaged','cold_reengaged','engaged','hot_engaged','hot_reengaged','reengaged','hot_new','hot_reactivated') then 'active'\n+# MAGIC when user_info.ifood_lifecycle_v2 in ('early_churn','late_churn','lost_churn','middle_churn') then 'churn'\n+# MAGIC when user_info.ifood_lifecycle_v2 in ('prospect') then 'prospect'\n+# MAGIC when user_info.ifood_lifecycle_v2 in ('new') then 'new_m0'\n+# MAGIC when user_info.ifood_lifecycle_v2 in ('reactivated') then 'reactivated_m0'\n+# MAGIC when user_info.ifood_lifecycle_v2 in ('cold_new','warm_new') then 'retention_m1_new'\n+# MAGIC when user_info.ifood_lifecycle_v2 in ('cold_reactivated','warm_reactivated') then 'retention_m1_reactivated'\n+# MAGIC else coalesce(user_info.ifood_lifecycle_v2,'Unclassified') end as lifecycle_ifood\n+# MAGIC\n+# MAGIC\n+# MAGIC from session_segmentation \n+# MAGIC join params\n+# MAGIC\n+# MAGIC where dt>=date_sub(dt_ref,2)\n+# MAGIC\n+# MAGIC )\n+# MAGIC\n+# MAGIC select distinct a.session_id, account_id, dt, dt_amsp, started_at, started_at_amsp, \n+# MAGIC coalesce(a.city,b.city) as city, coalesce(a.state,b.state) as state, lifecycle_bec, lifecycle_ifood,\n+# MAGIC\n+# MAGIC case when c.cidade is not null then coalesce(a.city,b.city) else \n+# MAGIC   case when e.estado is not null then concat('Outros - ',coalesce(a.state,b.state)) else 'Outros' end end as cidades_projeto\n+# MAGIC\n+# MAGIC\n+# MAGIC from sessions_ajustada a\n+# MAGIC left join centroids b\n+# MAGIC on a.centroid_id_ajustado=b.centroid_id\n+# MAGIC left join cidades_projeto c\n+# MAGIC on coalesce(a.city,b.city)=c.cidade\n+# MAGIC left join lifecycle d\n+# MAGIC on a.session_id=d.session_id\n+# MAGIC left join (select distinct estado from cidades_projeto) e\n+# MAGIC on coalesce(a.state,b.state)=e.estado\n+\n+# COMMAND ----------\n+\n+# MAGIC %md ### Convers\u00c3\u00a3o\n+# MAGIC _traffic_journey_\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Conversion\n+# MAGIC %sql\n+# MAGIC create or replace temp view trafego_conversao_bec_0 as \n+# MAGIC with trafego_merchants as (\n+# MAGIC\n+# MAGIC select distinct a.session_id, a.merchant_id, m.nome as merchant_name, a.order_id, b.gmv\n+# MAGIC\n+# MAGIC from traffic_journey a\n+# MAGIC join params\n+# MAGIC inner join merchants_projeto m\n+# MAGIC on a.merchant_id=m.merchant_id\n+# MAGIC left join orders_order_metrics b\n+# MAGIC on a.order_id=b.order_id\n+# MAGIC\n+# MAGIC where a.dt between date_sub(dt_ref,1) and date_add(dt_ref_fim,1)\n+# MAGIC and funnel_good_purchase is true\n+# MAGIC\n+# MAGIC -- qualify row_number() over (partition by a.order_id order by timestamp asc)=1\n+# MAGIC\n+# MAGIC ) \n+# MAGIC\n+# MAGIC select session_id, max(order_id is not null) as convertido,\n+# MAGIC count(distinct order_id) as pedidos, sum(gmv) as gmv, collect_list(order_id) as order_list\n+# MAGIC\n+# MAGIC from trafego_merchants\n+# MAGIC\n+# MAGIC group by 1\n+# MAGIC\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Temp view Conversion\n+df_trafego_conversao_bec_0 = spark.sql(\"\"\"SELECT * FROM trafego_conversao_bec_0\"\"\")\n+\n+validated_df_trafego_conversao_bec_0 = (\n+  datalake.dataframe2tempdataset(\n+    dataframe=df_trafego_conversao_bec_0, \n+    namespace=\"growth\", \n+    dataset=f\"trafego_conversao_bec_0{str(DT.datetime.now())}\", \n+    force_s3=True\n+  )\n+)\n+\n+validated_df_trafego_conversao_bec_0.createOrReplaceTempView(\"trafego_conversao_bec\")\n+\n+# COMMAND ----------\n+\n+# MAGIC %md ### Tr\u00c3\u00a1fego Home\n+# MAGIC _home_performance_\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Eventos Home\n+# MAGIC %sql\n+# MAGIC create or replace temp view trafego_home_bec as \n+# MAGIC\n+# MAGIC ------------------------- P\u00c3\u0081GINAS V\u00c3\u0081LIDAS DE B&C ------------------------- \n+# MAGIC\n+# MAGIC with page_ids as ( \n+# MAGIC\n+# MAGIC select * from paginas_totais_projeto\n+# MAGIC\n+# MAGIC ), cms_home_bec as (\n+# MAGIC\n+# MAGIC select home_id, home_title from homes_projeto\n+# MAGIC\n+# MAGIC -- select id as home_id, name as home_title\n+# MAGIC -- from cms_summary.unique_per_day\n+# MAGIC -- where level='page' and validity_end_date='9999-12-31'\n+# MAGIC -- and type.home_title_grouped='Bebidas'\n+# MAGIC -- and name NOT RLIKE '^[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}$'\n+# MAGIC\n+# MAGIC ------------------------- VISITAS EM P\u00c3\u0081GINAS COM EVENTO DE HOME ------------------------- \n+# MAGIC\n+# MAGIC ), trafego_home_0 as (\n+# MAGIC\n+# MAGIC select distinct a.dt, a.session_id, a.home_id, a.home_title, view_reference_id--, pdd.session_id is not null as converted\n+# MAGIC\n+# MAGIC from home_performance a\n+# MAGIC join params\n+# MAGIC inner join page_ids h\n+# MAGIC on a.home_id=h.page_id\n+# MAGIC -- left join (select distinct session_id from trafego_conversao_bec where pedidos>0) pdd\n+# MAGIC -- on a.session_id=pdd.session_id\n+# MAGIC\n+# MAGIC where a.dt between dt_ref and dt_ref_fim\n+# MAGIC\n+# MAGIC ), journey_0 as (\n+# MAGIC\n+# MAGIC select distinct session_id, timestamp, properties.home_id, \n+# MAGIC\n+# MAGIC case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then \n+# MAGIC\n+# MAGIC   case when entry_point='home_bu' then concat('Home - ',entry_point_details.root_home_title_grouped)\n+# MAGIC   when entry_point in ('search') then 'Busca'\n+# MAGIC   when entry_point in ('home_multi') then 'Home - Inicio'\n+# MAGIC   when entry_point in ('order_list') then 'Order List'\n+# MAGIC   when entry_point in ('pre_search') then 'Pre Busca'\n+# MAGIC   when entry_point in ('deeplink_root_internal_event') then 'Deeplink'\n+# MAGIC   when entry_point in ('other','no_entry_point_defined') then 'Outros'\n+# MAGIC   when entry_point in ('waiting') then 'Waiting'\n+# MAGIC   when entry_point in ('persistent_cart') then 'Carrinho Persistido'\n+# MAGIC   when entry_point in ('user_area') then 'Area do Usuario' else coalesce(entry_point,'Outros') end\n+# MAGIC\n+# MAGIC else \n+# MAGIC   case when source in ('callback_dynamic_home') then concat('Home - ',source_details.source_home_title_grouped)\n+# MAGIC   when source in ('view_user_area') then 'Area do Usuario'\n+# MAGIC   when source in ('callback_dynamic_page') then (case when cms.home_id is not null then 'Home B&C' else 'Subhome/Lista B&C' end)\n+# MAGIC   when source in ('view_order_list') then 'Order List' else 'Outros' end\n+# MAGIC\n+# MAGIC end as entry_point,\n+# MAGIC\n+# MAGIC\n+# MAGIC -- case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then a.entry_point_details.root_home_title_grouped\n+# MAGIC -- else a.source_details.source_home_title_grouped end as source_home_title_grouped,\n+# MAGIC\n+# MAGIC case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then a.entry_point_details.root_component_type\n+# MAGIC else a.source_details.source_component_type end as entry_point_details_component_type,\n+# MAGIC\n+# MAGIC case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then a.entry_point_details.root_card_name\n+# MAGIC else a.source_details.source_card_name end as entry_point_details_card_name,\n+# MAGIC\n+# MAGIC case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then a.entry_point_details.root_card_id\n+# MAGIC else a.source_details.source_card_id end as entry_point_details_card_id,\n+# MAGIC\n+# MAGIC case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then a.entry_point_details.root_list_name\n+# MAGIC else a.source_details.source_list_name end as entry_point_details_content_name,\n+# MAGIC\n+# MAGIC case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then a.entry_point_details.root_content_id\n+# MAGIC else a.source_details.source_content_id end as entry_point_details_content_id\n+# MAGIC\n+# MAGIC\n+# MAGIC from traffic_journey a\n+# MAGIC join params\n+# MAGIC left join cms_home_bec cms\n+# MAGIC on a.source_details.source_page_id=cms.home_id\n+# MAGIC where a.dt between date_sub(dt_ref,1) and date_add(dt_ref_fim,1)\n+# MAGIC and name='view_dynamic_home'\n+# MAGIC\n+# MAGIC ), trafego_home as (\n+# MAGIC\n+# MAGIC select app.dt_amsp, app.dt, a.session_id, cidades_projeto, min(coalesce(b.timestamp,app.started_at_amsp)) as dia_hora, lifecycle_bec, lifecycle_ifood,\n+# MAGIC\n+# MAGIC case when cms.home_id is not null then 'Home B&C' else 'Subhome/Lista B&C' end as acesso_bec,\n+# MAGIC\n+# MAGIC a.home_title as acesso_bec_details_page_alias,\n+# MAGIC a.home_id as acesso_bec_details_page_id,\n+# MAGIC null as acesso_bec_details_merchant_id,\n+# MAGIC null as acesso_bec_details_merchant_name,\n+# MAGIC\n+# MAGIC coalesce(b.entry_point, 'Outros') as entry_point, \n+# MAGIC entry_point_details_component_type, entry_point_details_card_name, entry_point_details_card_id,\n+# MAGIC entry_point_details_content_name, entry_point_details_content_id,\n+# MAGIC\n+# MAGIC convertido, pedidos, gmv, order_list\n+# MAGIC\n+# MAGIC from trafego_home_0 a\n+# MAGIC left join journey_0 b\n+# MAGIC on a.session_id=b.session_id and a.home_id=b.home_id\n+# MAGIC left join cms_home_bec cms\n+# MAGIC on a.home_id=cms.home_id\n+# MAGIC inner join sessoes_app app\n+# MAGIC on a.session_id=app.session_id\n+# MAGIC left join trafego_conversao_bec conv\n+# MAGIC on a.session_id=conv.session_id\n+# MAGIC\n+# MAGIC group by all\n+# MAGIC\n+# MAGIC ) \n+# MAGIC\n+# MAGIC select * from trafego_home\n+\n+# COMMAND ----------\n+\n+# MAGIC %md ### Tr\u00c3\u00a1fego Page\n+# MAGIC _page_performance_\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Eventos Page\n+# MAGIC %sql\n+# MAGIC create or replace temp view trafego_page_bec as \n+# MAGIC ------------------------- P\u00c3\u0081GINAS V\u00c3\u0081LIDAS DE B&C ------------------------- \n+# MAGIC\n+# MAGIC with page_ids as ( \n+# MAGIC\n+# MAGIC select * from paginas_totais_projeto\n+# MAGIC\n+# MAGIC ), cms_home_bec as (\n+# MAGIC\n+# MAGIC select home_id, home_title from homes_projeto\n+# MAGIC\n+# MAGIC -- select id as home_id, name as home_title\n+# MAGIC -- from cms_summary.unique_per_day\n+# MAGIC -- where level='page' and validity_end_date='9999-12-31'\n+# MAGIC -- and type.home_title_grouped='Bebidas'\n+# MAGIC -- and name NOT RLIKE '^[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}$'\n+# MAGIC\n+# MAGIC ------------------------- VISITAS EM P\u00c3\u0081GINAS COM EVENTO DE PAGE ------------------------- \n+# MAGIC\n+# MAGIC ), trafego_page_0 as (\n+# MAGIC\n+# MAGIC select distinct app.dt_amsp, app.dt, a.session_id, cidades_projeto, (min(view_page_at) + interval 1 second) as dia_hora, lifecycle_bec, lifecycle_ifood,\n+# MAGIC\n+# MAGIC case when cms.home_id is not null then 'Home B&C' else 'Subhome/Lista B&C' end as acesso_bec,\n+# MAGIC a.page_title as acesso_bec_details_page_alias,\n+# MAGIC a.page_id as acesso_bec_details_page_id,\n+# MAGIC null as acesso_bec_details_merchant_id,\n+# MAGIC null as acesso_bec_details_merchant_name,\n+# MAGIC\n+# MAGIC case when cms.home_id is not null then 'Home B&C' else a.page_id end as criterio_join_acesso\n+# MAGIC\n+# MAGIC from page_performance a\n+# MAGIC join params\n+# MAGIC inner join page_ids h\n+# MAGIC on a.page_id=h.page_id\n+# MAGIC left join cms_home_bec cms\n+# MAGIC on a.page_id=cms.home_id\n+# MAGIC inner join sessoes_app app\n+# MAGIC on a.session_id=app.session_id\n+# MAGIC\n+# MAGIC where a.dt between dt_ref and dt_ref_fim\n+# MAGIC group by all\n+# MAGIC\n+# MAGIC ------------------------- VISITAS EM P\u00c3\u0081GINAS COM EVENTO DE PAGE ------------------------- \n+# MAGIC\n+# MAGIC ), trafego_origem_pages as (\n+# MAGIC\n+# MAGIC select a.*, b.dia_hora as dia_hora_click,\n+# MAGIC coalesce(b.entry_point, 'Outros') as entry_point, \n+# MAGIC b.entry_point_details_component_type, b.entry_point_details_card_name, b.entry_point_details_card_id,\n+# MAGIC b.entry_point_details_content_name, b.entry_point_details_content_id,\n+# MAGIC\n+# MAGIC row_number () over (partition by a.session_id, a.criterio_join_acesso, a.dia_hora order by b.dia_hora desc) as rn_entry_point,\n+# MAGIC\n+# MAGIC convertido, pedidos, gmv, order_list\n+# MAGIC\n+# MAGIC from trafego_page_0 a\n+# MAGIC left join union_clicks b\n+# MAGIC on a.session_id=b.session_id and a.criterio_join_acesso=b.criterio_join_acesso and a.dia_hora>=b.dia_hora\n+# MAGIC left join trafego_conversao_bec conv\n+# MAGIC on a.session_id=conv.session_id\n+# MAGIC\n+# MAGIC ) \n+# MAGIC\n+# MAGIC select * from trafego_origem_pages where rn_entry_point=1\n+\n+# COMMAND ----------\n+\n+# MAGIC %md ### Tr\u00c3\u00a1fego Merchants Novos\n+# MAGIC _traffic_journey_\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Eventos Interaction\n+# MAGIC %sql\n+# MAGIC create or replace temp view trafego_interactions_bec as \n+# MAGIC with trafego_merchants as (\n+# MAGIC\n+# MAGIC select app.dt_amsp, app.dt, a.session_id, cidades_projeto, min(a.timestamp) as dia_hora, lifecycle_bec, lifecycle_ifood,\n+# MAGIC 'Merchant Interaction' as acesso_bec,\n+# MAGIC\n+# MAGIC name as acesso_bec_details_page_alias,\n+# MAGIC name as acesso_bec_details_page_id,\n+# MAGIC a.merchant_id as acesso_bec_details_merchant_id,\n+# MAGIC m.nome as acesso_bec_details_merchant_name,\n+# MAGIC\n+# MAGIC case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then \n+# MAGIC\n+# MAGIC   case when entry_point='home_bu' then concat('Home - ',entry_point_details.root_home_title_grouped)\n+# MAGIC   when entry_point in ('search') then 'Busca'\n+# MAGIC   when entry_point in ('home_multi') then 'Home - Inicio'\n+# MAGIC   when entry_point in ('order_list') then 'Order List'\n+# MAGIC   when entry_point in ('pre_search') then 'Pre Busca'\n+# MAGIC   when entry_point in ('deeplink_root_internal_event') then 'Deeplink'\n+# MAGIC   when entry_point in ('other','no_entry_point_defined') then 'Outros'\n+# MAGIC   when entry_point in ('waiting') then 'Waiting'\n+# MAGIC   when entry_point in ('persistent_cart') then 'Carrinho Persistido'\n+# MAGIC   when entry_point in ('user_area') then 'Area do Usuario' else coalesce(entry_point,'Outros') end\n+# MAGIC\n+# MAGIC else \n+# MAGIC   case when source in ('callback_dynamic_home') then concat('Home - ',source_details.source_home_title_grouped)\n+# MAGIC   when source in ('view_user_area') then 'Area do Usuario'\n+# MAGIC   when source in ('callback_dynamic_page') then (case when cms.home_id is not null then 'Home B&C' else 'Subhome/Lista B&C' end)\n+# MAGIC   when source in ('view_order_list') then 'Order List' else 'Outros' end\n+# MAGIC\n+# MAGIC end as entry_point,\n+# MAGIC\n+# MAGIC\n+# MAGIC -- case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then a.entry_point_details.root_home_title_grouped\n+# MAGIC -- else a.source_details.source_home_title_grouped end as source_home_title_grouped,\n+# MAGIC\n+# MAGIC case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then a.entry_point_details.root_component_type\n+# MAGIC else a.source_details.source_component_type end as entry_point_details_component_type,\n+# MAGIC\n+# MAGIC case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then a.entry_point_details.root_card_name\n+# MAGIC else a.source_details.source_card_name end as entry_point_details_card_name,\n+# MAGIC\n+# MAGIC case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then a.entry_point_details.root_card_id\n+# MAGIC else a.source_details.source_card_id end as entry_point_details_card_id,\n+# MAGIC\n+# MAGIC case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then a.entry_point_details.root_list_name\n+# MAGIC else a.source_details.source_list_name end as entry_point_details_content_name,\n+# MAGIC\n+# MAGIC case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then a.entry_point_details.root_content_id\n+# MAGIC else a.source_details.source_content_id end as entry_point_details_content_id,\n+# MAGIC\n+# MAGIC convertido, pedidos, gmv, order_list\n+# MAGIC\n+# MAGIC\n+# MAGIC from traffic_journey a\n+# MAGIC join params\n+# MAGIC inner join merchants_projeto m\n+# MAGIC on a.merchant_id=m.merchant_id\n+# MAGIC inner join sessoes_app app\n+# MAGIC on a.session_id=app.session_id\n+# MAGIC\n+# MAGIC left join homes_projeto cms\n+# MAGIC on a.source_details.source_page_id=cms.home_id\n+# MAGIC left join trafego_conversao_bec conv\n+# MAGIC on a.session_id=conv.session_id\n+# MAGIC\n+# MAGIC\n+# MAGIC where a.dt between date_sub(dt_ref,1) and date_add(dt_ref_fim,1)\n+# MAGIC and funnel_interaction is true\n+# MAGIC and funnel_intention is false\n+# MAGIC\n+# MAGIC group by all\n+# MAGIC\n+# MAGIC ) \n+# MAGIC\n+# MAGIC select * from trafego_merchants\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Eventos Intention\n+# MAGIC %sql\n+# MAGIC create or replace temp view trafego_intention_bec as \n+# MAGIC with trafego_merchants as (\n+# MAGIC\n+# MAGIC select app.dt_amsp, app.dt, a.session_id, cidades_projeto, min(a.timestamp) as dia_hora, lifecycle_bec, lifecycle_ifood,\n+# MAGIC 'Merchant Intention' as acesso_bec,\n+# MAGIC\n+# MAGIC name as acesso_bec_details_page_alias,\n+# MAGIC name as acesso_bec_details_page_id,\n+# MAGIC a.merchant_id as acesso_bec_details_merchant_id,\n+# MAGIC m.nome as acesso_bec_details_merchant_name,\n+# MAGIC\n+# MAGIC case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then \n+# MAGIC\n+# MAGIC   case when entry_point='home_bu' then concat('Home - ',entry_point_details.root_home_title_grouped)\n+# MAGIC   when entry_point in ('search') then 'Busca'\n+# MAGIC   when entry_point in ('home_multi') then 'Home - Inicio'\n+# MAGIC   when entry_point in ('order_list') then 'Order List'\n+# MAGIC   when entry_point in ('pre_search') then 'Pre Busca'\n+# MAGIC   when entry_point in ('deeplink_root_internal_event') then 'Deeplink'\n+# MAGIC   when entry_point in ('other','no_entry_point_defined') then 'Outros'\n+# MAGIC   when entry_point in ('waiting') then 'Waiting'\n+# MAGIC   when entry_point in ('persistent_cart') then 'Carrinho Persistido'\n+# MAGIC   when entry_point in ('user_area') then 'Area do Usuario' else coalesce(entry_point,'Outros') end\n+# MAGIC\n+# MAGIC else \n+# MAGIC   case when source in ('callback_dynamic_home') then concat('Home - ',source_details.source_home_title_grouped)\n+# MAGIC   when source in ('view_user_area') then 'Area do Usuario'\n+# MAGIC   when source in ('callback_dynamic_page') then (case when cms.home_id is not null then 'Home B&C' else 'Subhome/Lista B&C' end)\n+# MAGIC   when source in ('view_order_list') then 'Order List' else 'Outros' end\n+# MAGIC\n+# MAGIC end as entry_point,\n+# MAGIC\n+# MAGIC\n+# MAGIC -- case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then a.entry_point_details.root_home_title_grouped\n+# MAGIC -- else a.source_details.source_home_title_grouped end as source_home_title_grouped,\n+# MAGIC\n+# MAGIC case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then a.entry_point_details.root_component_type\n+# MAGIC else a.source_details.source_component_type end as entry_point_details_component_type,\n+# MAGIC\n+# MAGIC case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then a.entry_point_details.root_card_name\n+# MAGIC else a.source_details.source_card_name end as entry_point_details_card_name,\n+# MAGIC\n+# MAGIC case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then a.entry_point_details.root_card_id\n+# MAGIC else a.source_details.source_card_id end as entry_point_details_card_id,\n+# MAGIC\n+# MAGIC case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then a.entry_point_details.root_list_name\n+# MAGIC else a.source_details.source_list_name end as entry_point_details_content_name,\n+# MAGIC\n+# MAGIC case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then a.entry_point_details.root_content_id\n+# MAGIC else a.source_details.source_content_id end as entry_point_details_content_id,\n+# MAGIC\n+# MAGIC convertido, pedidos, gmv, order_list\n+# MAGIC\n+# MAGIC from traffic_journey a\n+# MAGIC join params\n+# MAGIC inner join merchants_projeto m\n+# MAGIC on a.merchant_id=m.merchant_id\n+# MAGIC inner join sessoes_app app\n+# MAGIC on a.session_id=app.session_id\n+# MAGIC\n+# MAGIC left join homes_projeto cms\n+# MAGIC on a.source_details.source_page_id=cms.home_id\n+# MAGIC left join trafego_conversao_bec conv\n+# MAGIC on a.session_id=conv.session_id\n+# MAGIC\n+# MAGIC\n+# MAGIC where a.dt between date_sub(dt_ref,1) and date_add(dt_ref_fim,1)\n+# MAGIC and funnel_intention is true\n+# MAGIC and funnel_purchase is false\n+# MAGIC\n+# MAGIC group by all\n+# MAGIC\n+# MAGIC ) \n+# MAGIC\n+# MAGIC select * from trafego_merchants\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Eventos Good Purchase\n+# MAGIC %sql\n+# MAGIC create or replace temp view trafego_good_purchase_bec as \n+# MAGIC with trafego_merchants as (\n+# MAGIC\n+# MAGIC select app.dt_amsp, app.dt, a.session_id, cidades_projeto, min(a.timestamp) as dia_hora, lifecycle_bec, lifecycle_ifood,\n+# MAGIC 'Merchant Good Purchase' as acesso_bec,\n+# MAGIC\n+# MAGIC name as acesso_bec_details_page_alias,\n+# MAGIC name as acesso_bec_details_page_id,\n+# MAGIC a.merchant_id as acesso_bec_details_merchant_id,\n+# MAGIC m.nome as acesso_bec_details_merchant_name,\n+# MAGIC\n+# MAGIC case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then \n+# MAGIC\n+# MAGIC   case when entry_point='home_bu' then concat('Home - ',entry_point_details.root_home_title_grouped)\n+# MAGIC   when entry_point in ('search') then 'Busca'\n+# MAGIC   when entry_point in ('home_multi') then 'Home - Inicio'\n+# MAGIC   when entry_point in ('order_list') then 'Order List'\n+# MAGIC   when entry_point in ('pre_search') then 'Pre Busca'\n+# MAGIC   when entry_point in ('deeplink_root_internal_event') then 'Deeplink'\n+# MAGIC   when entry_point in ('other','no_entry_point_defined') then 'Outros'\n+# MAGIC   when entry_point in ('waiting') then 'Waiting'\n+# MAGIC   when entry_point in ('persistent_cart') then 'Carrinho Persistido'\n+# MAGIC   when entry_point in ('user_area') then 'Area do Usuario' else coalesce(entry_point,'Outros') end\n+# MAGIC\n+# MAGIC else \n+# MAGIC   case when source in ('callback_dynamic_home') then concat('Home - ',source_details.source_home_title_grouped)\n+# MAGIC   when source in ('view_user_area') then 'Area do Usuario'\n+# MAGIC   when source in ('callback_dynamic_page') then (case when cms.home_id is not null then 'Home B&C' else 'Subhome/Lista B&C' end)\n+# MAGIC   when source in ('view_order_list') then 'Order List' else 'Outros' end\n+# MAGIC\n+# MAGIC end as entry_point,\n+# MAGIC\n+# MAGIC\n+# MAGIC -- case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then a.entry_point_details.root_home_title_grouped\n+# MAGIC -- else a.source_details.source_home_title_grouped end as source_home_title_grouped,\n+# MAGIC\n+# MAGIC case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then a.entry_point_details.root_component_type\n+# MAGIC else a.source_details.source_component_type end as entry_point_details_component_type,\n+# MAGIC\n+# MAGIC case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then a.entry_point_details.root_card_name\n+# MAGIC else a.source_details.source_card_name end as entry_point_details_card_name,\n+# MAGIC\n+# MAGIC case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then a.entry_point_details.root_card_id\n+# MAGIC else a.source_details.source_card_id end as entry_point_details_card_id,\n+# MAGIC\n+# MAGIC case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then a.entry_point_details.root_list_name\n+# MAGIC else a.source_details.source_list_name end as entry_point_details_content_name,\n+# MAGIC\n+# MAGIC case when source not in ('callback_dynamic_home','view_user_area','view_order_list','callback_dynamic_page') then a.entry_point_details.root_content_id\n+# MAGIC else a.source_details.source_content_id end as entry_point_details_content_id,\n+# MAGIC\n+# MAGIC convertido, pedidos, gmv, order_list\n+# MAGIC\n+# MAGIC from traffic_journey a\n+# MAGIC join params\n+# MAGIC inner join merchants_projeto m\n+# MAGIC on a.merchant_id=m.merchant_id\n+# MAGIC inner join sessoes_app app\n+# MAGIC on a.session_id=app.session_id\n+# MAGIC\n+# MAGIC left join homes_projeto cms\n+# MAGIC on a.source_details.source_page_id=cms.home_id\n+# MAGIC left join trafego_conversao_bec conv\n+# MAGIC on a.session_id=conv.session_id\n+# MAGIC\n+# MAGIC\n+# MAGIC where a.dt between date_sub(dt_ref,1) and date_add(dt_ref_fim,1)\n+# MAGIC and funnel_good_purchase is true\n+# MAGIC\n+# MAGIC group by all\n+# MAGIC\n+# MAGIC ) \n+# MAGIC\n+# MAGIC select * from trafego_merchants\n+\n+# COMMAND ----------\n+\n+# MAGIC %md ### Union Tr\u00c3\u00a1fego\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Trafego B&C Final\n+# MAGIC %sql\n+# MAGIC create or replace temp view trafego_bec_final_0 as\n+# MAGIC\n+# MAGIC with trafego_bec_0 as (\n+# MAGIC\n+# MAGIC select dt_amsp, dt, session_id, cidades_projeto, dia_hora, lifecycle_bec, lifecycle_ifood, 'query_home' as query_bec,\n+# MAGIC\n+# MAGIC acesso_bec,\n+# MAGIC acesso_bec_details_page_alias, acesso_bec_details_page_id,\n+# MAGIC acesso_bec_details_merchant_id, acesso_bec_details_merchant_name,\n+# MAGIC\n+# MAGIC entry_point, \n+# MAGIC entry_point_details_component_type, entry_point_details_card_name, entry_point_details_card_id,\n+# MAGIC entry_point_details_content_name, entry_point_details_content_id,\n+# MAGIC\n+# MAGIC coalesce(convertido,false) as convertido, coalesce(pedidos,0) as pedidos, coalesce(gmv,0) as gmv, order_list\n+# MAGIC\n+# MAGIC from trafego_home_bec\n+# MAGIC\n+# MAGIC union all\n+# MAGIC\n+# MAGIC select dt_amsp, dt, session_id, cidades_projeto, dia_hora, lifecycle_bec, lifecycle_ifood, 'query_page' as query_bec,\n+# MAGIC\n+# MAGIC acesso_bec,\n+# MAGIC acesso_bec_details_page_alias, acesso_bec_details_page_id,\n+# MAGIC acesso_bec_details_merchant_id, acesso_bec_details_merchant_name,\n+# MAGIC\n+# MAGIC entry_point, \n+# MAGIC entry_point_details_component_type, entry_point_details_card_name, entry_point_details_card_id,\n+# MAGIC entry_point_details_content_name, entry_point_details_content_id,\n+# MAGIC\n+# MAGIC coalesce(convertido,false) as convertido, coalesce(pedidos,0) as pedidos, coalesce(gmv,0) as gmv, order_list\n+# MAGIC\n+# MAGIC from trafego_page_bec\n+# MAGIC\n+# MAGIC union all\n+# MAGIC\n+# MAGIC select dt_amsp, dt, session_id, cidades_projeto, dia_hora, lifecycle_bec, lifecycle_ifood, 'query_interaction' as query_bec,\n+# MAGIC\n+# MAGIC acesso_bec,\n+# MAGIC acesso_bec_details_page_alias, acesso_bec_details_page_id,\n+# MAGIC acesso_bec_details_merchant_id, acesso_bec_details_merchant_name,\n+# MAGIC\n+# MAGIC entry_point, \n+# MAGIC entry_point_details_component_type, entry_point_details_card_name, entry_point_details_card_id,\n+# MAGIC entry_point_details_content_name, entry_point_details_content_id,\n+# MAGIC\n+# MAGIC coalesce(convertido,false) as convertido, coalesce(pedidos,0) as pedidos, coalesce(gmv,0) as gmv, order_list\n+# MAGIC\n+# MAGIC from trafego_interactions_bec\n+# MAGIC\n+# MAGIC union all\n+# MAGIC\n+# MAGIC select dt_amsp, dt, session_id, cidades_projeto, dia_hora, lifecycle_bec, lifecycle_ifood, 'query_intention' as query_bec,\n+# MAGIC\n+# MAGIC acesso_bec,\n+# MAGIC acesso_bec_details_page_alias, acesso_bec_details_page_id,\n+# MAGIC acesso_bec_details_merchant_id, acesso_bec_details_merchant_name,\n+# MAGIC\n+# MAGIC entry_point, \n+# MAGIC entry_point_details_component_type, entry_point_details_card_name, entry_point_details_card_id,\n+# MAGIC entry_point_details_content_name, entry_point_details_content_id,\n+# MAGIC\n+# MAGIC coalesce(convertido,false) as convertido, coalesce(pedidos,0) as pedidos, coalesce(gmv,0) as gmv, order_list\n+# MAGIC\n+# MAGIC from trafego_intention_bec\n+# MAGIC\n+# MAGIC union all\n+# MAGIC\n+# MAGIC select dt_amsp, dt, session_id, cidades_projeto, dia_hora, lifecycle_bec, lifecycle_ifood, 'query_good_purchase' as query_bec,\n+# MAGIC\n+# MAGIC acesso_bec,\n+# MAGIC acesso_bec_details_page_alias, acesso_bec_details_page_id,\n+# MAGIC acesso_bec_details_merchant_id, acesso_bec_details_merchant_name,\n+# MAGIC\n+# MAGIC entry_point, \n+# MAGIC entry_point_details_component_type, entry_point_details_card_name, entry_point_details_card_id,\n+# MAGIC entry_point_details_content_name, entry_point_details_content_id,\n+# MAGIC\n+# MAGIC coalesce(convertido,false) as convertido, coalesce(pedidos,0) as pedidos, coalesce(gmv,0) as gmv, order_list\n+# MAGIC\n+# MAGIC from trafego_good_purchase_bec\n+# MAGIC\n+# MAGIC )\n+# MAGIC\n+# MAGIC select /*+ BROADCAST(params) */\n+# MAGIC dt_amsp, dt, session_id, cidades_projeto, dia_hora, lifecycle_bec, lifecycle_ifood, query_bec,\n+# MAGIC\n+# MAGIC -- acesso_bec,\n+# MAGIC -- acesso_bec_details_page_alias, acesso_bec_details_page_id,\n+# MAGIC -- acesso_bec_details_merchant_id, acesso_bec_details_merchant_name,\n+# MAGIC\n+# MAGIC -- entry_point, \n+# MAGIC -- entry_point_details_component_type, entry_point_details_card_name, entry_point_details_card_id,\n+# MAGIC -- entry_point_details_content_name, entry_point_details_content_id,\n+# MAGIC\n+# MAGIC acesso_bec,\n+# MAGIC struct(acesso_bec_details_page_alias as page_alias, acesso_bec_details_page_id as page_id, acesso_bec_details_merchant_name as merchant_name, acesso_bec_details_merchant_id as merchant_id) as acesso_bec_details, \n+# MAGIC\n+# MAGIC entry_point, \n+# MAGIC struct(entry_point_details_component_type as component_type, entry_point_details_card_name as card_name, entry_point_details_card_id as card_id, \n+# MAGIC entry_point_details_content_name as content_name, entry_point_details_content_id as content_id) as entry_point_details,\n+# MAGIC\n+# MAGIC convertido, pedidos, gmv, order_list,\n+# MAGIC\n+# MAGIC row_number() over (partition by session_id order by dia_hora asc) as rn_session_bec,\n+# MAGIC row_number() over (partition by session_id, query_bec='query_good_purchase' order by dia_hora asc) as rn_convertion_bec,\n+# MAGIC row_number() over (partition by session_id, entry_point='Outros' order by dia_hora asc) as rn_session_bec_validos\n+# MAGIC\n+# MAGIC from trafego_bec_0\n+# MAGIC join params\n+# MAGIC where dt_amsp between dt_ref_amsp and date_sub(current_date(),1)\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Temp view Trafego B&C Final\n+df_trafego_bec_final_0 = spark.sql(\"\"\"SELECT * FROM trafego_bec_final_0\"\"\")\n+\n+validated_df_trafego_bec_final_0 = (\n+  datalake.dataframe2tempdataset(\n+    dataframe=df_trafego_bec_final_0, \n+    namespace=\"growth\", \n+    dataset=f\"trafego_bec_final_0{str(DT.datetime.now())}\", \n+    force_s3=True\n+  )\n+)\n+\n+validated_df_trafego_bec_final_0.createOrReplaceTempView(\"trafego_bec_final\")\n+\n+# COMMAND ----------\n+\n+# MAGIC %md # Final + Upload\n+\n+# COMMAND ----------\n+\n+DATA_PRODUCT = \"marketing_business_layer\"\n+DATASET = \"trafego_bec\"\n+\n+# COMMAND ----------\n+\n+notify_execution_start(f'{DATA_PRODUCT}.{DATASET}')\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Temp view\n+df_trafego_bec_final = spark.sql(\"\"\"SELECT * FROM trafego_bec_final\"\"\")\n+\n+validated_df_trafego_bec_final = (\n+  datalake.dataframe2tempdataset(\n+    dataframe=df_trafego_bec_final, \n+    namespace=\"growth\", \n+    dataset=f\"{DATASET}_{str(DT.datetime.now())}\", \n+    force_s3=True\n+  )\n+)\n+\n+# COMMAND ----------\n+\n+# DBTITLE 1,Carregar na business_layer\n+if validated_df_trafego_bec_final.count() > 0:\n+    validator = DataQualityDataframeValidator(\n+        dataframe=validated_df_trafego_bec_final,\n+        display_name=DATASET,\n+        slack_usernames=[\"@marcelo.cabral \", \"#data-growth-core-alerts\"],\n+    )\n+    check = (\n+        validator.isFormattedAsDate(\"dt_amsp\", \"yyyy-MM-dd\")\n+        .isNeverNull(\"dt_amsp\")\n+    )\n+\n+    validator.run(check)\n+    validated = validator.allConstraintsSatisfied()\n+\n+    if not validated:\n+        raise Exception(\"Validation failed!\")\n+\n+    write(\n+        dataframe=validated_df_trafego_bec_final,\n+        data_product=DATA_PRODUCT,\n+        dataset=DATASET,\n+        options={\n+            \"stage\": STAGE_GOLD,\n+            \"type\": \"batch\",\n+            \"mode\": \"append\",\n+            \"partitions\": [\"dt_amsp\"],\n+            \"dynamic_partitions\": True,\n+            \"merge_schema\": True,\n+        },\n+    )\n+else:\n+    raise Exception(\"Validation failed!\")\n+\n+# COMMAND ----------\n+\n+\n+\n+# COMMAND ----------\n+\n+# MAGIC %md # Valida\u00c3\u00a7\u00c3\u00a3o\n+\n+# COMMAND ----------\n+\n+# %sql\n+# select * \n+# from marketing_business_layer.trafego_bec\n+# where true\n+# and dt_amsp='2025-06-11'\n+\n+# COMMAND ----------\n+\n+# %sql\n+# select dt_amsp, --entry_point, \n+# count(distinct session_id) as session_bec\n+\n+# from marketing_business_layer.trafego_bec\n+# where true\n+# and rn_session_bec=1\n+# -- and dt_amsp='2025-06-11'\n+# group by all order by dt_amsp, session_bec desc\n+\n+# COMMAND ----------\n+\n+# %sql\n+# select dt_amsp, count(distinct case when entry_point='Outros' then session_id end) as sessions_outros, count(distinct session_id) as session_bec,\n+# count(distinct case when entry_point='Outros' then session_id end)*100/count(distinct session_id) as pct_sessions_outros\n+\n+# from marketing_business_layer.trafego_bec\n+# where true\n+# and rn_session_bec=1\n+# -- and dt_amsp='2025-06-11'\n+# group by all order by dt_amsp, session_bec desc\n+\n+# COMMAND ----------\n+\n+# %sql\n+# create or replace temp view trafego_bec_final as\n+\n+# select dt_amsp, dt, session_id, cidades_projeto, dia_hora, lifecycle_bec, lifecycle_ifood, query_bec, acesso_bec, \n+\n+# struct(acesso_bec_details_page_alias as page_alias, acesso_bec_details_page_id as page_id, acesso_bec_details_merchant_name as merchant_name, acesso_bec_details_merchant_id as merchant_id) as acesso_bec_details, \n+\n+# case when entry_point='\u00c3\u0081rea do Usu\u00c3\u00a1rio' then 'Area do Usuario' else entry_point end as entry_point, \n+\n+# struct(entry_point_details_component_type as component_type, entry_point_details_card_name as card_name, entry_point_details_card_id as card_id, \n+# entry_point_details_content_name as content_name, entry_point_details_content_id as content_id) as entry_point_details,\n+\n+# rn_session_bec, rn_session_bec_validos\n+\n+# from marketing_business_layer.trafego_bec\n+\n+\n+# COMMAND ----------\n+\n+# %sql\n+# select dt_amsp, entry_point, count(distinct session_id) as session_bec\n+\n+# from marketing_business_layer.trafego_bec\n+# where true\n+# and rn_session_bec=1\n+# and dt_amsp>='2025-06-14'\n+# group by all order by dt_amsp, session_bec desc\n\\ No newline at end of file",
    "model_output": {
      "executive_summary": "This code defines an ETL pipeline in PySpark for analyzing and exporting traffic data related to the Beverage & Convenience (B&C) vertical within the iFood platform. It reads data from various data products (consumer app sessions, user journey, cardstack CMS, etc.) and faster events. The script performs several transformations, including joining data from different sources, aggregating metrics, and deriving new features. The final transformed data is written back to a gold-layer table (`marketing_business_layer.trafego_bec`). Data quality validations are also included using the `DataQualityDataframeValidator` and temporary datasets are created for intermediate steps. Overall, the code aims to provide insights into how users are accessing B&C related pages/merchants within the iFood app.",
      "syntax_review": {
        "general": "The code appears to be syntactically correct. It uses standard PySpark syntax for data transformations, including `read`, `filter`, `createOrReplaceTempView`, `withColumn`, `join`, `group by`, and `union all`. The UDF usage with `extract_uuid_terms` and the subsequent structuring are correct.",
        "faster_event_usage": "Usage of `event_read` from `ifood_data.commons.faster_utils.faster_reader` looks standard for fetching data from Faster.",
        "spark_sql_usage": "The frequent use of `spark.sql` indicates a SQL-heavy approach. While functional, review if certain queries could be refactored into PySpark dataframe operations for potential performance gains. Ensure efficient SQL syntax is used within `spark.sql` blocks.",
        "temporary_datasets": "The consistent creation of temporary Datasets using `datalake.dataframe2tempdataset` is useful for auditing and debugging. However, excessive use might incur unnecessary storage costs and overhead.",
        "comments": "The code includes comments, primarily in the form of Databricks Markdown cells (`# MAGIC %md`). These comments provide context for the different sections of the script. However, in-line code comments are sparse and would improve readability. Pay attention to the use of accented characters and ensure character encoding is consistent."
      },
      "code_style": {
        "pep8_compliance": "The code generally adheres to PEP8 guidelines. However, there are some areas that could be improved:\n\n*   **Imports:** Import statements are spread throughout the script. It's recommended to group them at the beginning of the file.  There are also duplicate imports (e.g., `from datetime import date, timedelta`). Consolidate imports to avoid redundancy.\n*   **Line Length:** Some lines exceed the recommended 79 character limit. Consider breaking long lines to improve readability.\n*   **Naming Conventions:** Variable names are generally descriptive, although `df_00`, `df_01` etc. are not very informative. Use more meaningful names like `df_cities_info`.\n*   **Whitespace:** While largely good, some inconsistencies exist around operators and function calls.  Use consistent spacing for better readability.\n*   **Magic Numbers:** The code uses some magic numbers (e.g., `date_range_tabelas = 10`). Consider defining these as named constants with clear explanations.",
        "readability": "While the code is functional, readability can be enhanced through consistent formatting, more descriptive variable names, and additional in-line comments to explain complex logic.",
        "databricks_notebook_style": "The code is structured as a Databricks notebook with extensive use of markdown cells for headings and descriptions, which is standard practice."
      },
      "performance_analysis": {
        "spark_optimization": "Several aspects of the code could impact Spark performance:\n\n*   **Broadcast Joins:** The `/*+ BROADCAST(params) */` hint suggests attempts to optimize joins by broadcasting smaller tables. Verify that the tables being broadcasted are actually small enough to benefit from broadcasting.  Consider also enabling adaptive query execution (`spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")`) to allow Spark to automatically choose the best join strategy.\n*   **SQL vs. DataFrame Operations:** A significant portion of the logic is implemented using `spark.sql`. Evaluate whether converting some of these queries to DataFrame operations would improve performance. DataFrame operations often allow Spark's Catalyst optimizer to make more informed decisions.\n*   **Filtering Early:** The code filters data based on dates using `col('dt') >= date_sub(current_date(), date_range_leitura)`. Ensure that this filtering is applied as early as possible in the pipeline to reduce the amount of data processed in subsequent steps.\n*   **Partitioning:** The final data is partitioned by `dt_amsp`. Verify that this partitioning strategy aligns with how the data is typically queried. Using an appropriate partitioning scheme can significantly improve query performance.\n*   **Temporary Views:** The code creates numerous temporary views. While useful, excessive use of temporary views can lead to performance overhead. Consider caching frequently accessed DataFrames using `df.cache()` or `df.persist()` to avoid recomputation.\n*   **UDF performance:** Be wary of UDFs (User Defined Functions) as they might not be optimized as effectively by the Spark engine as built-in functions. The `extract_uuid_terms` function should be reviewed to ensure that it does not become a bottleneck.\n*   **Skewness in Data:** If any of the joins are experiencing significant skew, consider using techniques like salting to distribute the data more evenly across partitions.\n*   **Data Locality:**  Ensure that the data is being read from a location that is close to the Spark cluster to minimize network latency.  Consider using Delta Lake for storage, which can improve data locality and query performance.",
        "faster_performance": "Since Faster is heavily used, confirm that the events being accessed through `event_read` are optimized and indexed appropriately for efficient retrieval. Consider the date range specified in `event_read` and evaluate if smaller ranges can be used for faster processing.",
        "memory_management": "Monitor Spark's memory usage to identify potential memory leaks or excessive garbage collection. Adjust Spark's memory configuration as needed to optimize performance."
      }
    },
    "error": null
  }
]